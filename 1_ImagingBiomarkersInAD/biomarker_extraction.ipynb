{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imaging biomarker extraction walkthrough\n",
    "This notebook will go over some examples of extacting biomarkers from structural T1 weighted MR images and amyloid PET data. These are some of the most widely used biomarkers that are used in AD research. They are also measures used in the EBM.\n",
    "\n",
    "## Objectives\n",
    "After completing this notebook, you will be able to:\n",
    "* Understand how to open and interrogate meddical imaging data\n",
    "* Run basic image processing tasks like image registration and segmentation\n",
    "* Extract measurements of brain volumes and amyloid burden.\n",
    "\n",
    " There are many different publicly available, open-source software packges that you can use to obtain these measurements.  The specific techniques that we are using in this notebook are meant to demonstrate the key concepts involved in the processing pipelines, but they have not been optimised for this task. The exact decision of what software and pipeline to use for your research  will depend on what works best for your data and application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "First, we will import the packages that will be used in our notebook. We will be using the following packages:\n",
    "* [nibabel](https://nipy.org/nibabel/) - a handy package to work with neuroimaging data.\n",
    "* [Advanced Normalisation Toolbox (ANTs)](https://antspy.readthedocs.io/en/latest/index.html) - a popular set of tools that do registration, segmentation and other image processing techniques. \n",
    "* [Niwidgets](https://github.com/nipy/niwidgets) - a small package for making interactive viewers in Jupyter notebooks. This is useful for a quick view of an image.\n",
    "* [fsleyes](https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FSLeyes)(OPTIONAL)- a popular, fully featured image visualisation tool. It does have [support for Jupyter notebooks](https://open.win.ox.ac.uk/pages/fsl/fsleyes/fsleyes/userdoc/fsleyes_notebook.html). You might find this easier to interactively review your results rather than using the static plots provided here.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "import os\n",
    "import nibabel as nb\n",
    "import numpy as np\n",
    "from niwidgets import NiftiWidget\n",
    "import pandas as pd\n",
    "import ants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imaging basics\n",
    "The first step that we are going to do is load in an image and have a quick look at it. There are many different file formats used in medical imaging data, but the two most common are the _Digital Image and COmmunication in Medicine_ ([DICOM](https://www.dicomstandard.org/current))and the _Neuroimaging InFormatics Technology Initiative_ ([NIFTI](https://nifti.nimh.nih.gov/#:~:text=NIfTI%2D1%20is%20a%20new,MRI%20data%20analysis%20software%20packages.). In both cases, data is stored as a flat sheet (two-dimensional image data) or a cube (three-dimensional) where individual elements of the data are called pixels (2-D) or voxels (3-D). At every pixel or voxel, there is one or more numerical values associated with each element. These values could represent many different quantitites:\n",
    "* Scalar (single numeric value per voxel): intensity, a label identifying a structure\n",
    "* Vector/Array/Tensor (multiple numeric values): such as RGB colour values, a time series or multiple values acquired from the same sequence. \n",
    "\n",
    "Associated with this data is _metadata_ which describes key aspects of the image. This metadata could describe, among other things, how the data was acquired, information about the patient, how big the image is, how to map this image into the real world in the context of otehr images, and how the data for each pixel is represented.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opening an image\n",
    "Let's first load in an image stored in the Nifti file format into memory and see some basic information about it. One nice thing about Nifti is that you can _compress_ the image using gzip, which makes the image much smaller on your hard drive (roughoy 30-50% for images and up to 90-95% for binary masks), but most software packages can still read it without decomressing it (though it will still take up the same amount of memory when loaded up for viewing and analysis)\n",
    "\n",
    "**Note: If you want to run this yourself while on the course, download the data from the TeamCoders Team channel and change the data_root variable to match the location on your computer**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(176, 240, 256)\n",
      "('mm', 'sec')\n",
      "(1.1999999, 1.0, 1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davecash/anaconda3/envs/teamcoder_ebm/lib/python3.7/site-packages/nibabel/wrapstruct.py:212: DeprecationWarning:\n",
      "\n",
      "tostring() is deprecated. Use tobytes() instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_root = \"/Users/davecash/Data/IDEAS/TeamCoder_EBM/bids\"\n",
    "subject_id =  \"011-S-4906\"\n",
    "bids_desc = \"t1\"\n",
    "subject_root = os.path.join(data_root,\"sub-\"+subject_id)\n",
    "t1_img_name = os.path.join(subject_root,\"anat\",\"sub-\" + subject_id + \"_desc-\" + bids_desc + \".nii.gz\")\n",
    "t1_img = nb.load(t1_img_name)\n",
    "print(t1_img.shape)\n",
    "print(t1_img.header.get_xyzt_units())\n",
    "print(t1_img.header.get_zooms())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first line of output comes from printing out the shape field from the structure that we have creatingn by reading the Nifti image file using nb.load. It says we have a cube of imaging data in the file that contains 176 columns, 240 rows, and 256 slices.  The second row of output descibes the units used to describe the image. For distance we are using millimeters and for time we are using seconds. The final line of output indicates the size that each voxel represents, which is 1.2 x 1.0 x 1.0 mm. This inforatioon will be useful in a little while.\n",
    "\n",
    "Now lets find out what this image data contains. While we have loaded in the image, we have not yet loaded in the large cube of numbers that represents the image data. First we are going to do that by using the `get_fdata()` command. Once we have the large cube loaded in, we are  going to find a test voxel and ask for its intensity. We typically denote voxel locations as *i*, *j*, and *k*. Here we choose a test voxel location of (100,100,100) which means we want to know the intensity of the voxel at row 100 and column 100 within slice 100. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198.0\n"
     ]
    }
   ],
   "source": [
    "test_voxel = (100,100,100)\n",
    "t1_img_data = t1_img.get_fdata()\n",
    "print(t1_img_data[test_voxel])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python starts counting at **zero**, so this row 100 is actually the 101st row in the matrix. It also means that the indices used to access the voxels inside the array go from rows 0 to 175, not 1 to 176, and columns from 0 to 239, not 1 to 240. Watch what happens if I want to get the first row and column in the last slice if I think Python is counting from 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 256 is out of bounds for axis 2 with size 256",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9y/g1lfqkc525zbf3824tckr5m00000gp/T/ipykernel_16007/760722504.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfirst_voxel_in_last_slice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt1_img_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfirst_voxel_in_last_slice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index 256 is out of bounds for axis 2 with size 256"
     ]
    }
   ],
   "source": [
    "first_voxel_in_last_slice=(1,1,256)\n",
    "print(t1_img_data[first_voxel_in_last_slice])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move on from an individual voxel to the entire image. I want to know what the minimum and maximum intensities are over the entire image. How do I do that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "952.0\n"
     ]
    }
   ],
   "source": [
    "print(t1_img_data.min())\n",
    "print(t1_img_data.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From voxels to the real world\n",
    "Now we are able to look at voxels, but how do we know where they are in the real world? An MRI of a fruit fly or an elephant could contain 256 slices, but one image would have to represent a much larger size in the real world than the other. What if the elephant was sideways in the scanner, but the fruit fly had its head towards the top of the scanner and its tail at the bottom? Then the rows and columns would mean entirely different anatomical directions!\n",
    "\n",
    "Embedded in the images metadata are not only the dimensions of each voxel, but also an important transformation, or mapping, that tells us how to convert the voxel location *(i,j,k)* to the real-world coordinates of *(x,y,z)*. Understanding this mapping is very important, as we will be needing this information to align images later. This transformation is stored in the affine field of the nifti imaging strucutre as it is stored by nibabel. \n",
    "\n",
    "The real-world coordinate system tends to be defined according to the patient. The x-axis tends to go from patient left to patient right, the y axis tends to go from anterior to posterior, and the z-axis goes from top to bottom of the patient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   1.20000458    0.            0.         -103.64485931]\n",
      " [   0.            1.            0.         -102.7288208 ]\n",
      " [   0.            0.            1.         -152.76271057]\n",
      " [   0.            0.            0.            1.        ]]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "print(t1_img.affine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output shows the affine transformation which is represented as a four by four matrix. The first 3 rows and columns represent how we *rotate* and *scale* the image voxel to get it into real worlsd space. Here all the numbers off the main diagonal are 0, indicating that there is no rotation, so the image is in an orientation that is aligned with the cardinal axes of the scanner in the real world. That means that the rows of voxels go from left to right in the real world and the columns go from the patients front to the patients back, and finally the slices go from the top of the head down towards the neck. When there is no rotation, then it is really easy to understand how big each indivudal voxel, as the values in the diagonal will indicate how big the voxel is. These represent effectively that each voxel is 1.2 x 1.0 x 1.0mm in size. So when we move one voxel over in the X direction, we are moving 1.2mm in real world space, and in the other directions we are moving 1.0 mm in real world space. . \n",
    "\n",
    "The first three numbers in the final column that are all quite large represent the *translation* or shift we need to move the voxel into real-world space. The bottom row is always 0,0,0,1 and is used to make it easier to map one or thousands of points in one single matrix operation. \n",
    "\n",
    "How do we get the equivalent real-world coordinates from the voxel location? Through a process of matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_world = t1_img.affine.dot(list(test_voxel) + [1])\n",
    "print(real_world)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this means that the voxel (100,100,100) represents physical space (16.4 mm. -2.7 mm, -52.8 mm). When we have different kinds of images acquired in the scanning session or in other scanning sessions, then may have different orientations or different voxel sizes, then it makes it easier to compare and overlay the images together. We will see a bit more abotu that when we discuss registration. Now that we know a bit more about how to traverse the images and what they mean. Let's take a look at one! The niwdiget below plots it in your notebook. However, you can also look at it in the command line using the tool fsleyes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "niwidget = NiftiWidget(t1_img_name)\n",
    "niwidget.nifti_plotter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias correction\n",
    "Notice how in the white matter you can see some variation in intensity. It slowly varies from darker areas to lighter areas. This is due to the fact that the static magnetic field, known as $B_0$, is not perfectly constant everywhere. There are small fluctuations that slowly vary over the volume of the head that cause these subtle differences in intensity. These  differences can cause issues for subsequent processing routines needed to get accurate measures of brain volume. \n",
    "\n",
    "In order to reduce this effect, we employ a technique known as d _bias correction_, which  normalises the intensity across the image. For this tutorital, we are going to be using one of the most common bias correction algorithms out there, [N3](https://www.nitrc.org/docman/view.php/6/880/sled.pdf). Other software packages, like SPM, integrate bias correction with the[tissue segmetnation](#tissue-segmentation) step.  \n",
    "\n",
    "The code below will perform the bias correction and plot out images of the original image int he top row, the corrected image in the middle row and the difference between them in the bottom row. Notice how smooth the bias field is across the image. \n",
    "\n",
    "Let's also save the T1 N3 and T1_diff outputs so that we can see them in fsleyes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "t1_img = ants.image_read(t1_img_name)\n",
    "t1_n3 = ants.n3_bias_field_correction(t1_img)\n",
    "t1_diff = t1_img - t1_n3\n",
    "ants.plot_ortho_stack([t1_img,t1_n3,t1_diff])\n",
    "bids_desc = \"t1-n3\"\n",
    "t1_n3_name = os.path.join(subject_root,\"anat\",\"sub-\" + subject_id + \"_desc-\" + bids_desc + \".nii.gz\")\n",
    "ants.image_write(t1_n3,t1_n3_name)\n",
    "bids_desc = \"bias-field\"\n",
    "t1_diff_name = os.path.join(subject_root,\"anat\",\"sub-\" + subject_id + \"_desc-\" + bids_desc + \".nii.gz\")\n",
    "ants.image_write(t1_diff,t1_diff_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brain extraction\n",
    "Before we perform the [tissue segmentation](#tissue-segmentation) step, we are going to do a bit mroe pre-processing. Some tissue segmetnation algorithms require the removal of most of the non-brain structures (neck, eyes, skull, dura, eta) leacing only the brain parenchyma and surrounding cerebrospinal fluid.\n",
    "\n",
    "There are many different ways to do this, bt we are going to a very simple method to illustrate the process. More advanced methods, which might work better for your analysis that you want to do, are available.\n",
    "\n",
    "We are going to grap a standard atlas called MNI152. This comes with a handy mask. We are going to register this to our image and then use a predefined mask as our starting point.\n",
    "But look at the images below (and in fsleyes). You can see one is much bigger than the other and they are not in the same orientation.\n",
    "\n",
    "But before we do this and other steps, it's a good idea to do a step called _skull stripping_, where we remove all the bits from outside the brain. Again thare are lots of different ways to do this, but we are going to take a very simple approach for the time being. The following should show the original, the corrected, and then the difference image so that you can see the bias field. Also open these images up in dsleyes so that you can toggle back and forth and see the images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlasdir = os.path.join(data_root,\"atlas\")\n",
    "mni_img_file = os.path.join(atlasdir,\"MNI152_T1_1mm.nii.gz\")\n",
    "mni_img = ants.image_read(mni_img_file)\n",
    "mni_mask_file = os.path.join(atlasdir,\"MNI152_T1_1mm_brain_mask_dil.nii.gz\")\n",
    "mni_mask = ants.image_read(mni_mask_file)\n",
    "ants.plot_ortho_stack([mni_img,t1_n3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we are going to run a registration to get information from the standard atlas into the space of our image. We will discuss [registerion](#co-registration) later. Registration can be a tricky thing to do if we don't give it a good starting point, so we are going to first run an affine initialiser which will find a good starting point for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_tx_file = ants.affine_initializer(fixed_image = mni_img, moving_image = t1_n3, mask = mni_mask)\n",
    "init_tx = ants.read_transform(init_tx_file)\n",
    "t1_mni_init = ants.apply_transforms(fixed = mni_img, moving = t1_n3,transformlist=init_tx_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at the image and see if we have improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ants.plot_ortho_stack([mni_img,t1_mni_init])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That seems to have them more aligned! Now let's do the full registration just to see if we can improve it further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affine = ants.registration(fixed = mni_img, moving = t1_n3, type_of_transform = \"Affine\", initial_transform = init_tx_file, mask = mni_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what does that registration produce? Let's print out the return value to see what it holds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(affine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affine holds four key outputs (from the ANTS documentation):\n",
    "* _warpedmovout_: Moving image warped to space of fixed image. \n",
    "* _warpedfixout_: Fixed image warped to space of moving image. \n",
    "* _fwdtransforms_: Transforms to move from moving to fixed image. \n",
    "* _invtransforms_: Transforms to move from fixed to moving image.\n",
    "\n",
    "To see what the registration has done let us look at the mni image and the moxing image (our original image) transformed to MNI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ants.plot_ortho_stack([mni_img,affine['warpedmovout']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks really nice. Ants provides us both the _forward mapping_, moving our image into the coordinate system described by MNI and the _inverse mapping_, moving the MNI information back into our image. This allows use to take a mask from MNI and put it into our space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_mask = ants.apply_transforms(fixed = t1_n3, moving = mni_mask,transformlist=affine['fwdtransforms'],interpolator=\"nearestNeighbor\",whichtoinvert=[True])\n",
    "ants.plot_ortho(t1_n3,overlay=t1_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks pretty decent. We aren't missing any brain and so we can proceed with removing everything outside of this mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_brain = ants.mask_image(t1_n3,t1_mask)\n",
    "t1_mask_cleaned = ants.get_mask(t1_brain,cleanup=3)\n",
    "t1_brain_cleaned = ants.mask_image(t1_n3,t1_mask_cleaned)\n",
    "ants.plot_ortho_stack([t1_n3,t1_brain,t1_brain_cleaned])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tissue segmentation\n",
    "Next we are going to run a step of identifying tissue types within the brain. The brain is typically divided up into three primary tissue types: \n",
    "1. grey matter (GM),\n",
    "2. white matter (WM), and\n",
    "3. cerebrospinal fluid (CSF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tissue_seg = ants.atropos(a=t1_n3,x=t1_mask_cleaned,m=\"[0.3,1x1x1]\", i='Kmeans[3]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will view both the output and some statitics about this. You can see that background (LabelValue=0) has the lowest mean intensity, followed by CSF, then GM, and WM. The volume of the GM is about 574 ml (or 574373 $mm^3$). The images below that show a reasonable sebentation.\n",
    "We have saved the binary images so that you can look at them in fsleyes, but looking at the probability maps (shown below the segmentation) is also helpful in some cases as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tissue_stats = ants.label_stats(t1_n3,tissue_seg['segmentation'])\n",
    "print(tissue_stats)\n",
    "ants.plot_ortho_stack([t1_n3,tissue_seg['segmentation'],tissue_seg['probabilityimages'][0],\n",
    "tissue_seg['probabilityimages'][1],tissue_seg['probabilityimages'][2]])\n",
    "bids_desc = \"t1-bin-seg\"\n",
    "t1_seg_name =  os.path.join(subject_root,\"anat\",\"sub-\" + subject_id + \"_desc-\" + bids_desc + \".nii.gz\")\n",
    "ants.image_write(tissue_seg['segmentation'],t1_seg_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brain parcellation\n",
    "Another bit of labelling that is quite helpful is to identify brain regions for each voxel. These are really helpful when you wnat to define a region of interest (ROI) to obtain summary metrics for. This is primarily useful for calculating volumes of each structure, but also for multimodal analyiss when you want a high-resolution anatomical region defined to obtain summary metrics in low-resolution images where it is not possible to identify these regions directly.\n",
    "\n",
    "There are many ways to do this. Many of the most widely-used approaches take a considerable amount of time (anywhere from an hour to a whole day). For your workbook, consider running another model (FreeSurfer, FastSurfer, ANTS, etc) on your own.\n",
    "For the purpose of tdoay's demo, we are going to take a similar approach to the one used in the [brain extraction](#brain-extraction) section. This involves finding one labelled template atlas and registering it to our individual image.  \n",
    "\n",
    "Many people use the [Automated Anatomical Labelling (AAL)](https://www.gin.cnrs.fr/en/tools/aal/) atlas for this purpose. We will first open that data up and have a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine template, rigid + SYN and then transfer labels over. \n",
    "aal_template_file = os.path.join(atlasdir,\"AAL3v1_1mm.nii.gz\")\n",
    "aal_template = ants.image_read(aal_template_file).astype(\"uint32\")\n",
    "df_aal = pandas.read_table(os.path.join(atlasdir,\"AAL3v1_1mm.nii.txt\"))\n",
    "aal_template=ants.LabelImage(label_image=aal_template,label_info = df_aal)\n",
    "aal_img_file = os.path.join(atlasdir,\"single_subj_T1.nii\")\n",
    "aal_img = ants.image_read(aal_img_file)\n",
    "ants.plot_ortho(aal_img,overlay=aal_template)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we are not only going to do an affine registration but also a nonrigid registration to warp the tempalte to match our image. This is quite a challenge for the algorithm, as the AAL tempalte is based on a single healthy 27 year old, while our subjects come from an Alzheimer's Disease cohort where most of the population are over 70.\n",
    "\n",
    "This will take about four or five minutes to run. You are welcome to chop and change the setting to see if you can get improved registration.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_mask_dil = ants.iMath(t1_mask_cleaned,'MD',2)\n",
    "aal_affine_init_file = ants.affine_initializer(fixed_image = t1_n3,moving_image = aal_img)\n",
    "syn_aal = ants.registration(fixed=t1_n3,moving=aal_img,mask=t1_mask_dil,\n",
    "type_of_transform=\"SyN\",initial_transform=aal_affine_init_file,\n",
    "reg_iterations=(60,30,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ants.plot_ortho_stack([t1_n3,syn_aal['warpedmovout']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the most part, the iamges are aligned. Do you notice the wavy bits in the bright fat regions near the skull in the warped tempalte? That is likely becuase we have told the algorithm to focus on the brain and not the surrounding areas so these regions do not have to fit very well to get a good alignment. \n",
    "\n",
    "The next step is to use the results from the registration to warp the labels into our image. We are only using one subject to do this, where specific varioations in anatomy between your source of the labels and the image you want to label. In fact, many groups now use an approach where they have tens to hundreds of labelled templates that they do this process to for the image, resulting in the same number of candidate labellings. Differences are resolved using a label fusion step to take the consensus information from the different candidate labellings and generate a final parecellation. This process is often called _multi atlas label fusion_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_aal_label = ants.apply_transforms(fixed = t1_n3, moving = aal_template,transformlist=syn_aal['fwdtransforms'],interpolator=\"genericLabel\")\n",
    "ants.plot_ortho(t1_n3,overlay=t1_aal_label)\n",
    "bids_desc = \"aal-resampled\"\n",
    "t1_aal_resampled_name = os.path.join(subject_root,\"anat\",\"sub-\" + subject_id + \"_desc-\" + bids_desc + \".nii.gz\")\n",
    "ants.image_write(syn_aal['warpedmovout'],t1_aal_resampled_name)\n",
    "bids_desc = \"aal-labels\"\n",
    "t1_aal_label_name = os.path.join(subject_root,\"anat\",\"sub-\" + subject_id + \"_desc-\" + bids_desc + \".nii.gz\")\n",
    "ants.image_write(t1_aal_label,t1_aal_label_name)\n",
    "aal_stats = ants.label_stats(t1_n3,t1_aal_label)\n",
    "print(aal_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below plot comes from using the freeSurfer pipeline to do the parcellation. This approach takes 3-6 hours to run on machines, and you can see the substanial imprvoement in the images as a result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See if we can clean this up, perhaps nibabel to read in?\n",
    "# Also do the label image trick.\n",
    "fsdir=\"/Users/davecash/Data/freesurfer\"\n",
    "t1_fs_nu_name  = os.path.join(fsdir,subject_id.replace(\"-\",\"_\"),\"mri\",\"nu.mgz\")\n",
    "t1_fs_nu = ants.image_read(t1_fs_nu_name)\n",
    "t1_fs_aparc_name = os.path.join(fsdir,subject_id.replace(\"-\",\"_\"),\"mri\",\"aparc+aseg.mgz\")\n",
    "t1_fs_aparc = ants.image_read(t1_fs_aparc_name).astype(\"uint32\")\n",
    "ants.plot_ortho(t1_fs_nu,overlay=t1_fs_aparc,scale=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have parcellations. One valuable measure is the volume of each of the structures identified in the parcellation, just as we obtained from teh tissue segmentation above. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co Registration\n",
    "We have seen registration used above to match a standard atlas to our image. Now we are going to use it to align our PET data with our MRI data. \n",
    "Registration is used for many pipelines. The main goal is to determine the optimal mapping to align corresponding anatomy between it can be used to align different modalities together (for example PET to MR) to a standard space (such as MNI152) and to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bids_desc = \"av45-adni-preproc\"\n",
    "av45_name = os.path.join(subject_root,\"pet\",\"sub-\" + subject_id + \"_desc-\" + bids_desc + \".nii.gz\")\n",
    "av45 = ants.image_read(av45_name)\n",
    "pet_to_mri = ants.registration(fixed = t1_fs_nu,moving=av45,type_of_transform=\"Rigid\")\n",
    "ants.plot_ortho_stack([t1_fs_nu,pet_to_mri['warpedmovout']])\n",
    "fs_nu_in_pet = ants.apply_transforms(fixed=av45, moving=t1_fs_nu,whichtoinvert=[True],\n",
    "transformlist=pet_to_mri['fwdtransforms'],interpolator=\"bSpline\")\n",
    "bids_desc = \"t1-fs-nu\"\n",
    "fs_nu_in_pet_name = os.path.join(subject_root,\"anat\",\"sub-\" + subject_id + \"_space-av45_desc-\" + bids_desc + \".nii.gz\")\n",
    "ants.image_write(fs_nu_in_pet,fs_nu_in_pet_name)\n",
    "fs_aparc_in_pet = ants.apply_transforms(fixed=av45, moving=t1_fs_aparc,whichtoinvert=[True],\n",
    "transformlist=pet_to_mri['fwdtransforms'],interpolator=\"genericLabel\")\n",
    "bids_desc = \"t1-fs-aparc\"\n",
    "fs_aparc_in_pet_name = os.path.join(subject_root,\"anat\",\"sub-\" + subject_id + \"_space-av45_desc-\" + bids_desc + \".nii.gz\")\n",
    "ants.image_write(fs_aparc_in_pet,fs_aparc_in_pet_name)\n",
    "ants.plot_ortho(av45,overlay=fs_aparc_in_pet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images, and most importantly the labels look well aligned. So we will now proceed with getting the statistics out of these images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               LabelName       Volume      Mean       Min  \\\n",
      "LabelValue                                                                  \n",
      "0                                Unknown  7210465.000  0.264172  0.000000   \n",
      "2             Left-Cerebral-White-Matter   217522.125  2.094330  0.389447   \n",
      "4                 Left-Lateral-Ventricle    31347.000  0.808678  0.306855   \n",
      "5                      Left-Inf-Lat-Vent     1069.875  1.350232  0.946330   \n",
      "7           Left-Cerebellum-White-Matter    11120.625  1.706108  0.666365   \n",
      "...                                  ...          ...       ...       ...   \n",
      "2031                ctx-rh-supramarginal     9743.625  1.966591  0.822558   \n",
      "2032                  ctx-rh-frontalpole      931.500  1.886650  1.130699   \n",
      "2033                 ctx-rh-temporalpole     2467.125  1.600991  1.093412   \n",
      "2034           ctx-rh-transversetemporal      793.125  2.020394  1.669728   \n",
      "2035                       ctx-rh-insula     5923.125  1.828656  1.040724   \n",
      "\n",
      "                 Max  Variance  \n",
      "LabelValue                      \n",
      "0           3.940199  0.209200  \n",
      "2           2.747550  0.100929  \n",
      "4           1.867344  0.103623  \n",
      "5           1.880637  0.047485  \n",
      "7           2.437165  0.130964  \n",
      "...              ...       ...  \n",
      "2031        2.620082  0.131176  \n",
      "2032        2.536833  0.076296  \n",
      "2033        2.023842  0.035274  \n",
      "2034        2.486262  0.028029  \n",
      "2035        2.447796  0.077027  \n",
      "\n",
      "[109 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "pet_stats = ants.label_stats(av45,fs_aparc_in_pet)\n",
    "pet_stats[\"LabelValue\"] = pet_stats['LabelValue'].astype(\"int\")\n",
    "pet_stats = pet_stats.set_index(\"LabelValue\")\n",
    "df_aparc = pandas.read_csv(os.path.join(data_root,\"aparc_aseg_roi.csv\"),index_col=0)\n",
    "pet_stats = pet_stats.join(df_aparc)\n",
    "pet_stats = pet_stats[[\"LabelName\",\"Volume\",\"Mean\",\"Min\",\"Max\",\"Variance\"]]\n",
    "print(pet_stats)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1ca3c52c2c6a741687ac119195f853fd55cbbadb57e664fb666b72dd14734ec5"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
