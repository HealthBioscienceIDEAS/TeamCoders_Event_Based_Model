{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imaging biomarker extraction walkthrough\n",
    "This notebook will go over some examples of extacting biomarkers from structural T1 weighted MR images and amyloid PET data. These are some of the most widely used biomarkers that are used in AD research. They are also measures used in the EBM. The walkthrough will use some freely available, widely used open-source software packages as an example of how to perform this analysis. However, there are many other methods out there to choose from and determine which one would be right for your data and application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "First up, we need to import some packages into our notebook so that we can use them. For the purposes of this notebook, we will use [nipype](https://nipype.readthedocs.io/en/latest/) a handy workflow engine in Python that allows you to piece together different elements from different popular neuroimaging software packages. In addition to interrogate some information about the images, we will also use the packages [nibabel](https://nipy.org/nibabel/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "import os\n",
    "import nibabel as nb\n",
    "from niwidgets import NiftiWidget\n",
    "import ants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imaging basics\n",
    "The first step that we are going to do is load in an image and have a quick look at it. There are many different file formats used in medical imaging data, but the two most common are the _Digital Image and COmmunication in Medicine_ ([DICOM](https://www.dicomstandard.org/current))and the _Neuroimaging InFormatics Technology Initiative_ ([NIFTI](https://nifti.nimh.nih.gov/#:~:text=NIfTI%2D1%20is%20a%20new,MRI%20data%20analysis%20software%20packages.). In both case, data is stored as a flat sheet (two-dimensional image data) or a cube (three-dimensional) where individual elements of the data are called pixels (2-D) or voxels (3-D) where one or more numerical values are associated with each element. These values could represent the intensity, a time series of intensities, color values (RGB), vectors, or other different measurements. Associated with this data is _metadata_ which describes key aspects of the data. This metadata could describe, among other things, how the data was acquired, information about the patient, how big the image is, where in physical space of the scanner the image is located, and how data in each pixel is represented.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opening an image\n",
    "Let's first load in an image stored in the Nifti file format into memory and see some basic information about it. One nice thing about Nifti is that you can _compress_ the image using gzip, which makes the image much smaller on your hard drive (roughoy 30-50% for images and up to 90-95% for binary masks), but most software packages can still read it without decomressing it (it will still take up the same amount of memory on your computer when you load it in as part of a program though)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"/Users/davecash/Data/IDEAS\"\n",
    "img_name = os.path.join(data_root,\"nifti\",\"02-021-11_20151120130726_2.nii.gz\")\n",
    "img = nb.load(img_name)\n",
    "print(img.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line of output comes from printing out the field shape from the Nifti image file. It says we have a cube of imaging data in the file that contains 208 rows, 256 columns, and 256 slices.  Now lets find out what this image data contains. I'm going to find a test voxel and ask for its intensity. We typically denote voxel locations as *i*, *j*, and *k*. I have chosen voxel location (100,100,100) which means I want to know the intensity of the voxel at row 100 and column 100 within slice 100. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_voxel = (100,100,100)\n",
    "img_data = img.get_fdata()\n",
    "print(img_data[test_voxel])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python starts counting at **zero**, so this row 100 is actually the 101st row in the matrix. It also means that the indexes used to access the voxels inside the array go frrom rows 0 to 207, not 1 to 208, and columns from 0 to 255, not 1 to 256. Watch what happens if I want to get the first row and column in the last slice if I think Python is counting from 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_voxel=(1,1,256)\n",
    "print(img_data[last_voxel])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what range of intensities are there in this image volume? We can get that by asking what the minimum and maximum intensities are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img_data.min())\n",
    "print(img_data.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are able to look at voxels, but how do we know where they are in the real world? An MRI of a fruit fly or an elephant could contain 256 slices, but one image would have to represent a much larger size in the real world than the other. What if the elephant was sideways in the scanner, but the fruitfly had its head towards the top of the scanner and its tail at the bottom? Then the rows and columns might mean entirely different anatomical directions!\n",
    "\n",
    "Embedded in the images metadata are not only the dimensions of each voxel, but also an important transformation, or mapping, that tells us how to convert the voxel location *(i,j,k)* to the real-world coordinates of *(x,y,z)*. Understanding this mapping is very important, as we will be needing this information to align images later. This transformation is stored in the affine field. \n",
    "\n",
    "The real-world coordinate system tends to be defined according to the patient. The x-axis tends to go from patient left to patient right, the y axis tends to go from anterior to posterior, and the z-axis goes from top to bottom of the patient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img.affine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we see here is a four by four matrix. The first 3 rows and columns represent how we *rotate* and *scale* the image voxel to get it into real worlsd space. Here all the numbers off the main diagonal are 0, indicating that there is no rotation going on. That means that the rows of voxels go from left to right in the real world and the columns go from the patients front to the patients back, and finally the slices go from the top of the head down towards the neck. When there is no rotation, than it is really easy to understand how big each indivudal voxel, as the values in the diagonal will indicate how big the voxel is. These represent effectively that each voxel is 1.1 x 1.1 x 1.1mm in size. So when we move one voxel over, we are moving 1.1mm in real world space. \n",
    "\n",
    "The first three numbers in the final column that are all below -100 represent what *translation* or shift we need to do to get the voxel into real-world space. The bottom row is always 0,0,0,1 and is used to make the mapping easier. \n",
    "\n",
    "How do we get the equivalent real-world coordinates from the voxel location? Through a process of matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_world = img.affine.dot(list(test_voxel) + [1])\n",
    "print(real_world)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this means that the voxel (100,100,100) represents physical space (-3.8mm. -12.7mm, -24.1mm). When we have different kinds of images acquired in the scanning session or in other scanning sessions, then may have different orientations or different voxel sizes, then it makes it easier to compare and overlay the images together. We will see a bit more abotu that when we discuss registrati0on. Now that we know a bit more about how to traverse the images and what they mean. Let's take a look at one! The niwdiget below plots it in your notebook. However, you can also look at it in the command line using the tool fsleyes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "niwidget = NiftiWidget(img_name)\n",
    "niwidget.nifti_plotter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias correction\n",
    "Notice how in the white matter you can see some variation in intensity. It slowly varies from darker areas to lighter areas. This is due to the fact that the static magnetic field, known as B_0, is not perfectly constant everywhere. There are small fluctuations that slowly vary over the volume of the head that cause these subtle differences in intensity.\n",
    "\n",
    "The good news is that there are many established _bias correction_ techniques that can normalise the intensity across the image. For ease of this tutorital, we are going to be using the one in the [ANTS pacage](https://antspy.readthedocs.io/en/latest/index.html). Some software packages integrate it with the[tissue segmetnation](#tissue-segmentation) step. We are going to run one of the most commonly used bias correction algorithms called N3. \n",
    "\n",
    "But before we do this and other steps, it's a good idea to do a step called _skull stripping_, where we remove all the bits from outside the brain. Again thare are lots of different ways to do this, but we are going to take a very simple approach for the time being. The following should show the original, the corrected, and then the difference image so that you can see the bias field. Also open these images up in dsleyes so that you can toggle back and forth and see the images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img = ants.image_read(img_name)\n",
    "img_n3 = ants.n3_bias_field_correction(img)\n",
    "diff_img = img - img_n3\n",
    "ants.plot_ortho_stack([img,img_n3,diff_img])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brain extraction\n",
    "Before we perform the [tissue segmentation](#tissue-segmentation) step, we are going to do a bit mroe pre-processing. Some tissue segmetnation algorithms require the removal of most of the non-brain structures (neck, eyes, skull, dura, eta) leacing only the brain parenchyma and surrounding cerebrospinal fluid.\n",
    "\n",
    "There are many different ways to do this, bt we are going to a very simple method to illustrate the process. More advanced methods, which might work better for your analysis that you want to do, are available.\n",
    "\n",
    "We are going to grap a standard atlas called MNI152. This comes with a handy mask. We are going to register this to our image and then use a predefined mask as our starting point.\n",
    "But look at the images below (and in fsleyes). You can see one is much bigger than the other and they are not in the same orientation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsldir = os.getenv(\"FSLDIR\")\n",
    "mni_img_file = os.path.join(fsldir,\"data\",\"standard\",\"MNI152_T1_1mm.nii.gz\")\n",
    "mni_img = ants.image_read(mni_img_file)\n",
    "mni_mask_file = os.path.join(fsldir,\"data\",\"standard\",\"MNI152_T1_1mm_brain_mask_dil.nii.gz\")\n",
    "mni_mask = ants.image_read(mni_mask_file)\n",
    "ants.plot_ortho_stack([mni_img,img_n3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we are going to run a registration to get information from the standard atlas into the space of our image. We will discuss [registerion](#co-registration) later. Registration can be a tricky thing to do if we don't give it a good starting point, so we are going to first run an affine initialiser which will find a good starting point for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_tx_file = ants.affine_initializer(fixed_image = mni_img, moving_image = img_n3, mask = mni_mask)\n",
    "init_tx = ants.read_transform(init_tx_file)\n",
    "print(init_tx.parameters)\n",
    "img_resampled_init = ants.apply_transforms(fixed = mni_img, moving = img_n3,transformlist=init_tx_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at the image and see if we have improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ants.plot_ortho_stack([mni_img,img_resampled_init])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That seems to have them more aligned! Now let's do the full registration just to see if we can improve it further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affine = ants.registration(fixed = mni_img, moving = img_n3, type_of_tranform = \"Affine\", initial_transform = init_tx_file, mask = mni_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what does that registration produce? Let's print out the return value to see what it holds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(affine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affine holds four key outputs (from the ANTS documentation):\n",
    "* _warpedmovout_: Moving image warped to space of fixed image. \n",
    "* _warpedfixout_: Fixed image warped to space of moving image. \n",
    "* _fwdtransforms_: Transforms to move from moving to fixed image. \n",
    "* _invtransforms_: Transforms to move from fixed to moving image.\n",
    "\n",
    "To see what the registration has done let us look at the mni image and the moxing image (our original image) transformed to MNI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ants.plot_ortho_stack([mni_img,affine['warpedmovout']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks really nice. Ants provides us both the _forward mapping_, moving our image into the coordinate system described by MNI and the _inverse mapping_, moving the MNI information back into our image. This allows use to take a mask from MNI and put it into our space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_mask = ants.apply_transforms(fixed = img_n3, moving = mni_mask,transformlist=affine['invtransforms'],interpolator=\"nearestNeighbor\")\n",
    "ants.plot_ortho(img_n3,overlay=img_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks pretty decent. We aren't missing any brain and so we can proceed with removing everything outside of this mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_brain = ants.mask_image(img_n3,img_mask)\n",
    "ants.plot_ortho_stack([img_n3,img_brain])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tissue segmentation\n",
    "Next we are going to run a step of identifying tissue types within the brain. The brain is typically divided up into three primary tissue types: \n",
    "1. grey matter (GM),\n",
    "2. white matter (WM), and\n",
    "3. cerebrospinal fluid (CSF)\n",
    "Some of these tissue types can be further broken down (for example cortical GM versus subcortical GM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tissue_seg = ants.atropos(a=img_n3,x=img_mask,m=\"[0.3,1x1x1]\", i='Kmeans[3]')\n",
    "print(tissue_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ants.plot_ortho_stack([img_n3,tissue_seg['segmentation']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brain parcellation\n",
    "Another bit of labelling that is quite helpful is to identify brain regions for each voxel. These are really helpful when you wnat to define a region of interest (ROI) to obtain summary metrics for. This is primarily useful for calculating volumes of each structure, but also for multimodal analyiss when you want a high-resolution anatomical region defined to obtain summary metrics in low-resolution images where it is not possible to identify these regions directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co-Registration\n",
    "We have seen registration used above to match a standard atlas to our image. Now we are going to use it to align our PET data with our MRI data. \n",
    "Registration is used for many pipelines. The main goal is to determine the optimal mapping to align corresponding anatomy between it can be used to align different modalities together (for example PET to MR) to a standard space (such as MNI152) and to"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1ca3c52c2c6a741687ac119195f853fd55cbbadb57e664fb666b72dd14734ec5"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
