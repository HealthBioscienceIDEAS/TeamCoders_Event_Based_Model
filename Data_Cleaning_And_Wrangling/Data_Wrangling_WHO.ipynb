{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a href=\"https://pandas.pydata.org/\">\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/ed/Pandas_logo.svg/2560px-Pandas_logo.svg.png\" width=\"300px\">\n",
    "</a>\n",
    "\n",
    "\n",
    "# Data Wrangling with pandas\n",
    "\n",
    "\n",
    "\n",
    "## Objectives\n",
    "This is the demonstration notebook for the first task. The facilitators will present a lecture that is based around this notebook. Before performing the first task as a team, we recommend you first go through this walkthrough together, as it will show a lot of techniques and code that will be useful in the task.  \n",
    "\n",
    "\n",
    "The main objective of this walkthrough is to familiarize you with how to use the [pandas python package](https://pandas.pydata.org/) for essential tasks of discovering, cleaning, structuring, and visualizing tabular data. In particular, this notebook will cover:\n",
    "* The basic concept behind the core data structures within Pandas: [Series](https://pandas.pydata.org/docs/user_guide/dsintro.html#series) and [DataFrames](https://pandas.pydata.org/docs/user_guide/dsintro.html#dataframe)\n",
    "* How to process, organize and clean data stored in DataFrames\n",
    "* Visualization of data stored within a DataFrame.\n",
    "\n",
    "After completing this notebook you will be able to:\n",
    "* Understand key features of the Pandas library.\n",
    "* Import tabular data from spreadsheets into Pandas\n",
    "* Understand your data, its size, features and its structure.\n",
    "* Identify missing values and figure out how to handle them\n",
    "* Ensure your data is in the right format for processing and analysing\n",
    "* Group, summarize, and transform variables in your dataset\n",
    "* Visualise data using Matplotlib library\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do we mean by data wrangling?\n",
    "Data wrangling is the process of reading in the raw data and then cleaning and processing it so that the resulting information is in suitable format for analysis. Part of the data wrangling process includes data discovery, data cleaning (identifying and then removing any erroneous data), handling missing data, transforming variables, and visualizing the resulting data.\n",
    "\n",
    "Data wrangling is often needed before any quantitative analysis can begin; however, it is often the most time-consuming and tedious part of the process. There are many different skills and actions that are required as part of data wrangling, many of which are illustrated in the example code below. Many of these steps are part of the functionality that Pandas provides. \n",
    "\n",
    "Here are some common steps that are considered part of data wrangling:\n",
    "\n",
    "* **Discovering**: \n",
    "Reviewing and understanding the input data to better understand its structure and what variables will be useful for your problem \n",
    "* **Structuring**:\n",
    "Standardising the format for disparate types of data and make the data usable for automated or semiautomated data analysis. \n",
    "\n",
    "* **Cleaning** : \n",
    "There are often missing or implausible values in a dataset. These occur for a number of reasons and they could adversely affect the reliability and repeatability of an analysis. Cleaning techniques are identify and in some cases correct these instances so that they can be handled appropriately in subsequent analyses.\n",
    "\n",
    "* **Enriching**\n",
    "You are pretty familiar with the data at this point. This is the moment to ask yourself if you want to enhance the data. Are you looking to add other data to it?\n",
    "\n",
    "\n",
    "*  **Validating** \n",
    "This step involves iterative programming steps that authenticate your dataâ€™s quality and safety. For example, you may have problems if your data is not clean or enriched and the attributes are not distributed evenly.\n",
    "\n",
    "Many of these different stages of data wrangling require you to visualize the data in some form other than the base tabular data. Visualisation libraries such as [Matplotlib](https://matplotlib.org/), and [Seaborn](https://seaborn.pydata.org/) are often used to plot data as well as perform some basic statistical analysis. For example, one simple way of identify outliers for cleaning is to plot the data points and visually identify points that appear to be well outside the plausible range of the rest of the data for a given variable. \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [1. Importing pandas and reading in data](#1.-Importing-pandas-and-reading-in-data)\n",
    "* [2. Discovering and reviewing the data](#2.-Discovering-and-reviewing-the-data)\n",
    "* [3. Handling missing data](#3.-Handling-missing-data)\n",
    "* [4. Checking the data representation](#4.-Checking-the-data-representation)\n",
    "* [5. Grouping data](#5.-Grouping-data)\n",
    "* [6. Sorting data](#6.-Sorting-data)\n",
    "* [7. Summarizing](#7.-Summarizing-data)\n",
    "* [8. Combining and merging data sets](#8.-Combining-and-merging-data-sets)\n",
    "* [9. Visualizing the data](#9.-Visualizing-the-data)\n",
    "* [10. Transforming variables](#10.-Transforming-variables)\n",
    "* [11. More exploration and visualisation](#11.-More-exploration-and-visualisation)\n",
    "\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Importing pandas and reading in data\n",
    "### 1.1 Importing packages\n",
    "The first thing that we need to do is to import the Python packages we are going to need. This includes pandas, [numpy](https://numpy.org/) (which Pandas uses to store data in the two-dimensional tabular structure), and [matplotlib](https://matplotlib.org/), a popular Python package used for data visualisation.\n",
    "\n",
    "For convenience, we will import these packages using the shortname (`pd`,`np`,`plt`) for each that has become convention, so that we don't always have to provide the package name each time we use some functionality from the package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.2 Reading in data\n",
    "For this notebook, we are using a dataset based on statistics from the World Health Organisation. There are many sources where you might get data from that you will then want to work with using pandas. The most common is to open a file containing tabular data. The most well-known format for tabular data are Excel spreadsheets. Another commonly used format are plain-text *comma separated values* files with the extension `csv`. Each line of a csv file represents a row, and each column is separated by a comma. \n",
    "\n",
    "Besides local files, you may also get data by querying a database or requesting data from the web. \n",
    "\n",
    "All of these sources generate tabular data that can be stored within pandas DataFrames.\n",
    "\n",
    "For the purpose of this notebook, we have assumed you have [downloaded the data](https://liveuclac.sharepoint.com/:x:/r/sites/TeamCodersEventsPlanning/Shared%20Documents/General/WHO/who_case_statistics_modified3.csv?d=wec3e29c1cb9d43779f37cc43bf9eed3b&csf=1&web=1&e=wy7e84) and it is stored to a local file. If you dowloaded the zipfile that we have provided you or cloned the github repo, the data file will already be there.\n",
    "\n",
    "pandas has many different functions that will read in data depending on the data format. Below, we will read the filename and store it the variable `input_file` and then use the [read_csv](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) function to read this file \n",
    "and store it into a DataFrame, under a variable named `df_raw`. Here the df stands for DataFrame, so that you know what is stored in the variable.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We are assuming that you have downloaded the spreadsheets\n",
    "# into a subdirectory just below this notebook called \"data\"\n",
    "# If you have not downloaded the file into the data subdirectory\n",
    "# please ammend the first line so that input_file contains\n",
    "# the full location (path) of the file\n",
    "input_file = \"data/who_case_statistics_modified3.csv\" \n",
    "df_raw = pd.read_csv(input_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## 2. Discovering and reviewing the data \n",
    "Before we can work with the data, we need to know about more about the contents of the data. Here are some core functions that can help you start to review the structure and contents of your DataFrame. \n",
    "* `pandas.DataFrame.head()` - prints out the first *n* rows of the DataFrame \n",
    "* `pandas.DataFrame.tail()` - prints out the last *n* rows of the DataFrame\n",
    "\n",
    "You can understand a bit more of the data structure using the following commands:\n",
    "* `pandas.DataFrame.info()` - summary information (column names, counts, data types) \n",
    "* `pandas.DataFrame.describe()` - provides basic summary statistics for each variable\n",
    "\n",
    "Some basic low-level information about the DataFrame can be extracted with the following attributes:\n",
    "* `pandas.DataFrame.shape` - provides the number of rows and columns in the DataFrame\n",
    "* `pandas.DataFrame.dtypes` - indicates the datatype (string, float, integer, etc) used to store the data for each variable\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Review the data\n",
    "Use the method <b>head()</b> to display the first five rows of the dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_raw.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how there are headers at the top, which match up with the column names. Each row also has an _index_, which starts at 0 and counts up for this spreadsheet. Indices are like headers, but for rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "Use the tail method to see the last part of the data.\n",
    "How many lines do you see?\n",
    "Use the the tail method again to see the last 20 lines of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "Use the info method to gather information  about the data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "* The data frame contains 27840 rows and 8 columns.\n",
    "* The data frame is an object of class pandas.DataFrame.\n",
    "* Columns that hold only numeric data are stored as floats or integers\n",
    "* Columns that hold text or a mixture of data are stored as objects. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "Write any other observation that you see about the data. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Understanding headers\n",
    "How did pandas know how to name the columns? If you opened up the file in a text editor, you would see that the first row contains these names. When you run `read_csv`, pandas assumes that the column names (or _headers_) are located in the first row. If that is not the case, you can specify which row they can be found by using the `header` argument. \n",
    "\n",
    "However, if there are not any column names in the file, then it is important to tell pandas that, lest you lose actual data. This can be done by setting `header` to `None`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the data - but assuming no headers\n",
    "df_raw = pd.read_csv(input_file, header=None)\n",
    "# Now look at the head, does it look correct?\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also look at the info, does it look correct?\n",
    "df_raw.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Selecting the columns and getting the values\n",
    "The way to to select columns is to write the \n",
    "`dataframe['column_name']`. When selecting one column only,\n",
    "pandas will return a pandas data type known as a Series.\n",
    "DataFrames consist of a collection of Series.\n",
    "\n",
    "The following command selects the country column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the headers back\n",
    "df_raw = pd.read_csv(input_file)\n",
    "countries=df_raw['country']\n",
    "print(countries)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that, even though we select one column `country` that the indices on the left remain. As mentioned these are like headers for rows and they are there even if there is only a one column and it is a Series instead of DataFrame. \n",
    "\n",
    "We can see how many unique countries are in this column, and how many times each country is found in the data, using the `pandas.DataFrame.value_counts()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw['country'].value_counts() #select the column with data_raw['country'] then add the method .value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we just want a list of all countries found in this column, regardless of how many times they appear, we can use the `unique()` function. The output from this will be a one-dimensional numpy array.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_array = df_raw['country'].unique()\n",
    "print(country_array)\n",
    "print(type(country_array))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to convert the numpy array to a Python list, we will use the numpy `tolist()` method. In case the country list is not alphabetically sorted, we can use the `sorted()` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_list = country_array.tolist() # Convert a numpy array to a list\n",
    "sorted_country_list = sorted(country_list)\n",
    "print(country_list)\n",
    "print(sorted_country_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many total countries? Use len to find the length of the list\n",
    "len(country_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have looked at how to explore categorical data. Now let's look at numeric data. We can get the mean of a column using the `mean()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_numbers = df_raw['cases_no']\n",
    "case_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_mean = df_raw['cases_no'].mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have received an error here, and the key reason for this can be seen at the bottom of the printout of the variable `cases_no`. This column is of the data type `object`. This typically denotes a mixture of different data, both numeric and non-numeric. As a result, pandas cannot work out a mean. We will focus on handling missing and incorrectly coded data in the next section.\n",
    "\n",
    "*** "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Handling missing data\n",
    "In the above printout of `cases_no`, we can see the code `NaN` in some of the rows, which is a special numeric value in numpy and pandas, which means \"Not A Number\". When Pandas reads in the file, it identifies empty values, as well as certain strings (there are many, but for example \"`#N/A`\" and \"`None`\"), as missing data and replaces this with the `NaN` code. However, especially when data has not yet been cleaned, there may be many other ways that a missing value is indicated, such as the string `Missing` or a specific numeric code. There are many ways of handling missing data, but the first step in all of them is to identify the missing data and make sure they are labelled as such by pandas. In this section, we will identify all of the missing values and prepare the format of our DataFrame to be ready for analysis.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Identifying missing data\n",
    "Pandas has a function to test whether data has this special value of `NaN`:  `isnull()` or `isna()`. The opposite function to detect non-missing values is `notnull()` or `notna()`. The output from these functions are boolean (i.e. `True` or `False`).  \n",
    "\n",
    "The following code will check every cell in our DataFrame and tell us how many have missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.isnull().value_counts() # Indicates with True that there are Nulls and counts the values that are numbers "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output shows all of the combinations with missing data. Two columns - `cases_no` and `HDI for year` - have some missing values because the `isnull()` test has returned `True`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data = df_raw.isnull() # find data that are null and return 'True' if is null and 'False' if it is not\n",
    "missing_data.head(20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we wanted to determine the number of missing values for each column separately? We can use a loop to get the number of missing values found in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loop will go through each column in the data frame\n",
    "for column in missing_data:  \n",
    "    print(column)\n",
    "    print(missing_data[column].value_counts())\n",
    "    print('----')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The length of the dataframe is 27,840 and the column `cases_no` has 23,575 rows where `isnull()` is `False`, indicating that there is numeric data, and it has 4,265 values of `True` (27,840 - 23,575). \n",
    "\n",
    "The `HDI for year` column has many more observations with missing values - 19,472 in total. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Identifying non-numeric data in a column\n",
    "Most of the missing values have already been identified and appropriately labelled by pandas, but since the `cases_no` column is not a numeric datatype, there must still be some non-numeric data stored in it. All the values are currently represented by strings, so we can find all rows where the string represents data that is non-numeric. We will use the method `str.isdigit()`, which will generate a set of Boolean values much like our test above with `isnull()` did. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_numeric = ( df_raw['cases_no'].str.isdigit() == False)\n",
    "non_numeric.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Notice how this method has only found twenty values, far less than we found with the `isnull()` method. That is because `NaN` is considered a numeric value. However, these are the values that are keeping us from having numeric data and being able to get a simple mean out. So we need to find out what they are and handle them appropriately. \n",
    " \n",
    "We will use a handy method in pandas called _boolean indexing_. The following code will only select rows in the column `cases_no` where the variable `non_numeric` is equal to `True`. Thus it will return only twenty rows, the ones that have non-numeric data, meaning that we can easily see these 20 rows from the 27,840 in total. From these selected rows, we will use the method `unique()` to remove duplicate values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_raw['cases_no'][non_numeric])\n",
    "print(\n",
    "    f\"The non-numeric data contains the following values:\"\n",
    "    f\"{df_raw['cases_no'][non_numeric].unique()}\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So these twenty rows have two unique values: `Null` and `Unknown`. These two strings are not automatically identified and labelled as missing by pandas when reading in the file and now we can set these values as missing. We can use the `replace()` function to remove these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw['cases_no'] = df_raw['cases_no'].replace('Null',np.nan)\n",
    "df_raw['cases_no'] = df_raw['cases_no'].replace('Unknown',np.nan)\n",
    "non_numeric = ( df_raw['cases_no'].str.isdigit() == False)\n",
    "non_numeric.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "The output should indicate that `cases_no` no longer has non-numeric data. How can you tell that? Put your answer in the blank markdown cell below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "What datatype is `cases_no` now? Will the mean work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your answer to Exercise 5 here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we still need to have pandas convert this to a numeric type that will allow us to do more analysis with it.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw['cases_no'] = df_raw['cases_no'].apply(pd.to_numeric)\n",
    "print(df_raw['cases_no'].dtype)\n",
    "df_raw['cases_no']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the mean now work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_mean = df_raw['cases_no'].mean()\n",
    "print(cases_mean)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Dealing with missing data\n",
    "Now that we have identified what data is missing, how should we handle that for our analysis? There are many options, and the right one will be based on the data and what assumptions that can be made on why the data is missing. For some of the more complex options, you should consult with a statistician before implementing.\n",
    "\n",
    "* Drop data\n",
    "  * Drop the whole row\n",
    "  * Drop the whole column\n",
    "* Replace data - often known as imputation\n",
    "  * Replace it with the mean\n",
    "  * Replace it by the mode\n",
    "  * Replace it based on other functions\n",
    "  * Replace it with multiple guesses from the expected distribution (multiple imputation)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Dropping a column with lots of missing data.\n",
    "To remove a column we use the `pandas.DataFrame.drop()` method. The input argument should be the list of column names that you want to drop, and axis argument should be set to one to tell pandas that you are wanting to drop columns, not rows. \n",
    "\n",
    "We don't need the column `HDI for year` for our analysis, and since there are a lot of missing values there, we can just drop it from the data frame. \n",
    "\n",
    "To avoid writing over the original data_frame, we will save the output to a new variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_hdi = df_raw.drop(['HDI for year'], axis = 1)                         \n",
    "df_no_hdi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we turn to the other variable in the data frame that had missing values `cases_no`. Now that all of the missing values are coded properly, we can identify all of the missing data with the `isnull()` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = df_no_hdi['cases_no'].isnull()\n",
    "missing_values.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have correctly assigned those twenty values as `NaN`, there are twenty more `True` values when we call `isnull()`.\n",
    "\n",
    "To do a _complete case analysis_, we can simply use the `dropna()` function. Even though `cases_no` is now the only column with missing data, we tell dropna to only consider it when decided which rows to drop. We use `axis=0` to tell it to drop rows, not columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete = df_no_hdi.dropna(axis=0, subset=['cases_no'])\n",
    "df_complete.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the new data frame only contains 23555 entries, which is how many rows with non-missing `cases_no` values. Let's take a look at the actual data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a bit odd that we have only 23,555 rows now but some of the rows in the spreadsheet still have their old index (27839). We can fix that by using `reset_index(drop=True)`. Reset index restarts the indexes at 0 and counting up. The `drop=True` tells pandas that we do not want to retain the old index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete = df_complete.reset_index(drop=True)\n",
    "df_complete"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Replacing missing values\n",
    "Replacing or inputting missing data can be a complex subject. Here we are only going to show the simplest options, where we replace missing values with a single value. One option is to replace the missing value with a mean.\n",
    "\n",
    "In order to do that we make a copy of the DataFrame and then use the boolean indexing technique to select only cells which have been identified as missing. We access only these rows in the `cases_no` column using the `loc[rows,cols]` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_cases_replaced = df_no_hdi.copy()\n",
    "# With loc, we are giving it the rows we want to select - \n",
    "# missing values which are true where cases_no is NaN\n",
    "# and the columns we want to select, in this case, just cases_no\n",
    "df_cases_replaced.loc[missing_values,'cases_no'] = cases_mean\n",
    "df_cases_replaced"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the next to last row has the mean instead of a specific number. But notice that means we have a rather large number of cases (218) for a very small population of Grenada (11760), so maybe the mean is not the best option.\n",
    "\n",
    "Another option is to replace a missing value with the most frequent value, also known as the mode. This can be done with the `mode()` function. As there can be more than one mode (i.e. in the case of a tie between multiple values), it will return a pandas Series. However, we can see here that there is only one mode (1), so we will replace using that value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cases_mode = df_no_hdi['cases_no'].mode()\n",
    "print(f\"The mode is {cases_mode}\")\n",
    "df_cases_replaced.loc[missing_values,'cases_no'] = cases_mode.iloc[0]\n",
    "df_cases_replaced"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now instead of the mean (217.8), the missing value is the penultimate row is replaced with the mode (1.0). While that might seem more plausible for a small country like Grenada, it may be less plausible for a larger country, like Mexico.\n",
    "\n",
    "As mentioned earlier, imputation of missing data is a complex subject, and if you don't have a good approach, often removing the data is the best option, so moving forward we will be working with complete case data stored in `df_complete`\n",
    "\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Checking the data representation\n",
    "\n",
    "As was seen above with the `cases_no` column, having each column in a proper data format really matters. Let's make sure that we don't have similar problems with other columns. We first get information about the datatype representing each column using the `info()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would expect `age` and ` gdp_for_year ($) ` to be numeric, but they are not. Let's start with `age` and remind ourselves why they may not be numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete['age'].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK! So these don't represent years, but a band of ages. How many bands are we talking about?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_complete['age'].unique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case age is not a continuous numeric variable, but a _categorical_ variable with six different age bands. The best way to represent this in pandas is with a [Categorical](https://pandas.pydata.org/docs/user_guide/categorical.html) data type, similar to `factor` in R. These age bands also have a clear ordering, but because of the way pandas sorts text, they are not in that order (look at where the 5-14 year old group is). So when we convert to a Categorical, we need to tell pandas what order these values should be in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_bands_sorted = [\n",
    "    '5-14 years','15-24 years','25-34 years', \n",
    "    '35-54 years','55-74 years','75+ years'  \n",
    "    ]\n",
    "df_complete['age'] = pd.Categorical(df_complete['age'],\n",
    "                                    categories=age_bands_sorted)\n",
    "df_complete['age']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how this variable now has a dtype of `category` and the categories are sorted so that the levels go up with age?\n",
    "\n",
    "We can do the same thing for the `sex` column. With only two values we don't really need to specify an order.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now what is going on with the column representing gdp? Let's again use head to take a look at some data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete['gdp_for_year ($)'].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Why did that not work? Because if we look carefully, we find that there are unhelpful spaces at the beginning and end of the column name. Let's fix that before we get any further by using the `rename()` function, where we provide a dictionary where the key is the old column name, and the value is the new column name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete = df_complete.rename(columns={' gdp_for_year ($) ':'gdp_for_year_usd'})\n",
    "df_complete.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now let's look at our freshly renamed column and see if it contains numeric data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete['gdp_for_year_usd'].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks numeric to me? Does pandas agree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_is_numeric = (df_complete['gdp_for_year_usd'].str.isdigit()==False)\n",
    "gdp_is_numeric.value_counts()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas say that none of these values are numeric. The reason is because we did not tell pandas to expect commas to separate thousands, millions, etc. \n",
    "\n",
    "In fact, if we had spotted this before we read in the file, we could add the argument `thousands=','` to our `read_csv` call. However, this is still easily fixed by using a string operation to remove all commas from this column, in a manner similar to how we converted the \"`Null`\" and \"`Unknown`\" strings to `NaN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First replace the string comma ',' empty space''. Then typecasting the column to integers\n",
    "numeric_gdp = df_complete['gdp_for_year_usd'].str.replace(',', '').astype(int)\n",
    "# Breaking it up into two lines to make it easier to see\n",
    "df_complete['gdp_for_year_usd'] = numeric_gdp\n",
    "# Confirm that this is now an integer\n",
    "df_complete['gdp_for_year_usd']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Wonderful clean data!**\n",
    "\n",
    "Now we have finally obtained the cleaned dataset with no missing values with all data in its proper format.\n",
    "We can save this data set as it is now the 'clean' data set which is ready for processing.\n",
    "Note that we don't need the row index saved as it isn't especially useful, so we are using the `index=False` to say to Pandas not to save it as part of the csv file. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete.to_csv(r'who_case_statistics_clean_data.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## 5. Grouping data\n",
    "Now that our data is cleaned, we will move on to what operations we can perform on the DatFrame. Often we want to group series of observations together and perform an operation on them. This is accomplished by the `groupby()` function, which allows users to:\n",
    "* **split** data into groups according to some value in the variable (like country)\n",
    "* **apply** a function to all of the observations within the group\n",
    "* **combine** the resulting options into a new aggregated data frame.\n",
    "\n",
    "The output of the `groupby()` function is a [GroupBy object](https://pandas.pydata.org/docs/user_guide/groupby.html). We will group the data frame by `country` and from that grouped object, I will select the country of preference by using `.get_group('selection')`. Let's select the data for the United Kingdom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_country= df_complete.groupby(['country'])\n",
    "df_uk=df_by_country.get_group('United Kingdom')\n",
    "df_uk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this grouping, we would be summarizing the data for each country, combining data from all thirty years, all age groups, and both sexes. This is probably too coarse of a summary for most interesting statistical analysis. \n",
    "\n",
    "So we can group by multiple columns at a time, and perform operations that summarize across these groups. For example, let's say we want the total number of cases each year per country across all the age groups and both male and female individuals. \n",
    "\n",
    "To do this we would use the aggregating function `sum()` to sum up the `cases_no` and `population` columns over all rows for a given country and year.  \n",
    "\n",
    "However, `gdp_for_year_usd` is constant for a given country or year, so summing is not appropriate, as it would make the resulting gdp much larger than it actually is. \n",
    "\n",
    "Instead, we will use the aggregating function `first()` which just takes the value from the first row in the group, which is fine because it is the same value for all rows (If you don't believe me just look up at the output from `df_uk`, or take the standard deviaton of all the rows and see what happens.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupedby_country_yr = df_complete.groupby(['country','year'])\n",
    "df_by_country_yr = groupedby_country_yr[['cases_no','population']].sum()\n",
    "df_by_country_yr['gdp_for_year_usd'] = groupedby_country_yr['gdp_for_year_usd'].first()\n",
    "df_by_country_yr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `age` and `sex` column is no longer included in the output because it has been compressed by the group level aggregation. \n",
    "\n",
    "Also look at our indices. We now no longer have the numbers going from 0 to n, but instead we two columns of indices. One representing the country and another representing the year. If you don't like these values as indices and would prefer them to remain as columns use can use the parameter `as_index=False` when you run `groupby()`.\n",
    "\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Sorting data\n",
    "Observations (rows) can be rearranged so that they are sorted, either by numerical value or alphabetical order using the method `sort_values()`. Let's see which year had the most cases in the UK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uk = df_by_country_yr.loc['United Kingdom',['cases_no','population']]\n",
    "df_uk.sort_values('cases_no')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## 7. Summarizing data  \n",
    "The methods `max()`, `min()`,`mean()`,`std()` can be used to find the summary statistics maximum, minimum, mean and standard deviation. The `pandas.DataFrame.describe()` function shows all of these statistics (and more!) at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uk['cases_no'].max() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the print() function to print the numbers \n",
    "# that you calculate with the .max(), .min(). mean() .std() \n",
    "# with this print you can control the number of decimal places\n",
    "# for the mean and the standard deviation using the :0.1f bit\n",
    "print(f\"The mean number of cases in the UK is {df_uk['cases_no'].mean():0.1f}\")\n",
    "print(f\"The standard deviation of cases in the UK is {df_uk['cases_no'].std():0.1f}\")\n",
    "print(f\"The minimum number of cases in the UK is {df_uk['cases_no'].min()}\")\n",
    "print(f\"The maximum number of cases in the UK is {df_uk['cases_no'].max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uk.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6 \n",
    "\n",
    "Using the similar approaches from obove, obtain the same stats for Greece and store it in a variable `df_greece`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total number of cases per year for Greece and \n",
    "# produce the summary statistics \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## 8. Combining and merging data sets \n",
    "Often we will need to combine data from two different data sets together. There are two primary operations to combine data:\n",
    "* **concatenating** two datasets, typically vertically to provide additional observations of the same data.\n",
    "* **joining/merging** linked records from two different files that share a common key or a common set of keys. This usually adds new variables to the same observation.\n",
    "\n",
    "There are different types of joins (_inner_, _outer_, _left_, _right_) that differ on what values to keep. A schematic is seen below. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![merge](./fig/fig_merge.jpg)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Merging data\n",
    "Let's say we want to do a side by side comparison of cases in the UK and Greece. First let's extract them again from the main dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_country = df_complete.groupby(['country'])\n",
    "df_uk = df_by_country.get_group('United Kingdom')\n",
    "df_greece = df_by_country.get_group('Greece')\n",
    "print(df_uk.head())\n",
    "print(df_greece.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the `reset_index()` function again, so that both DataFrames start with new indices from 0, rather than the original rows of the master DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uk = df_uk.reset_index(drop=True)\n",
    "df_greece = df_greece.reset_index(drop=True)\n",
    "print(df_uk)\n",
    "print(df_greece)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use the `merge` function in pandas to combine them. We have a few things to think about here. \n",
    "* One data frame needs to be the left data frame (see above figure) and the other needs to be the right data frame. \n",
    "* No matter if the record is found in just the left data frame, just the right data frame, or both, we want to keep it. That is called an _outer_ join and is specified by setting the parameter to `how='outer'`.\n",
    "* The `year`, `age`, and `sex` columns are the _keys_ that we will want to match for the row by row comparison. These are fed into the `on` parameter of merge.\n",
    "* The other three variables `cases_no`, `population`, and `gdp_for_year_usd` are the variables that we are going to compare. \n",
    "* We can find out if there was a match or not by setting `indicator=True`. This creates a new column called `_merge` that indicates if the observation was found in the left, the right, or both.\n",
    "* We can add suffixes so that we know where the column data comes from with the `suffixes` parameter. With this, the country columns become irrelevant and can be removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uk_greece = pd.merge(df_uk, df_greece,\n",
    "                        how='outer',on=['year','age','sex'],\n",
    "                        suffixes=['_uk','_greece'],indicator=True) \n",
    "# Since suffixed in the merge, we can drop the countries\n",
    "df_uk_greece = df_uk_greece.drop(columns = ['country_uk','country_greece'])\n",
    "df_uk_greece"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how at the bottom of the DataFrame, the `_merge` column is set to `left_only`, meaning that data for 5-14 year olds in 2015 was only found in the `df_uk` but not `df_greece`. For those value missing from `df_greece`, you can see that the value is set to missing (`NaN`).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Concatenating data\n",
    "Concatenation is a bit different from merging. Concatenating is about bolting on additional rows (if the columns are the same in both) or additional columns (if the rows are the same). Unlike merging, which only works with two data frames at a time, concatenate can merge multiple (i.e two or more) dataframes together. \n",
    "\n",
    "Let's use our two data frames `df_uk` and `df_greece` again and try to concatenate vertically. \n",
    "If we were to run the following command\n",
    "```python\n",
    "df_concatenated = pandas.concat([df1, df2])\n",
    "```\n",
    "\n",
    "then it would _vertically_ concatenates the dataframes (df1 and df2), and the default results will look like the picture:\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![concatenate1](./fig/fig1_concat.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example assumes that your column names are the same betweeen the two dataframes.  If your column names are different while concatenating along rows (axis 0), then by default the columns will also be added, and NaN values will be filled in as applicable."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![concatenate2](./fig/fig2_concat.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenation is usually performed vertically to add more observations (rows) of the same columns. However, you can also concatenate horizontally along the same row indices by passing the parameter `axis=columns`.\n",
    "```python\n",
    "df_concatenated = pandas.concat([df1, df2], axis='columns')\n",
    "```\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![concatenate3](./fig/fig3_concat.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets concatenate our datasets and view them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uk_greece_vconcat = pd.concat([df_uk, df_greece])\n",
    "df_uk_greece_vconcat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This results that the datasets are being stacked with `df_uk` on top of `df_greece`. They are concatenated by default on the \"row axis\" or axis=0. If you look carefully the indices repeat. The first thirty rows of the UK have indices of 0 to 29, and the first thirty rows of the Greece data also have indices from 0 to 29. If you want to avoid this, you can either use `reset_index` after the concatenation, or you can use the `ignore_index=True` parameter while performing the concatenation.\n",
    "\n",
    "Now let's try a horizontal concatenation using the `axis=1` parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uk_greece_hconcat = pd.concat([df_uk, df_greece], axis =1)\n",
    "print(df_uk_greece_hconcat.columns)\n",
    "df_uk_greece_hconcat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the dataframes are stacked side by side, which is probably unhelpful, because they both have the same column names, which will be difficult and confusing if you are trying to index a specific column.\n",
    "\n",
    "***\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualizing the data\n",
    "\n",
    "There is only so much you can learn from the data, especially when there are large numbers of variables and observations, from looking at it in tabular form. Effective data visualization is an essential tool with many applications, from initial exploration to disseminating findings. There are various libraries that you can use in Python to visualize your data, with the two most popular being are [Matplotlib](https://matplotlib.org/), and [Seaborn](https://seaborn.pydata.org/). Here, we will use the Matplotlib library to do some example visualizations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  `%matplotlib inline` is what is known as a magic function in Jupyter. It tells Jupyter Lab to display the resulting figures in the notebook right below the code\n",
    "\n",
    "  Below we plot both the UK and Greece cases (summed across ages and sex) against the year using [plot](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html) function. You can call this multiple times to produce multiple plots on the same axis. The call signature is `plot(x,y,fmt)` where `x` is the vector of x coordinate values, `y` is the vector of y values, and `fmt` is the formatting of the points (marker shape, marker color, line between points). \n",
    "\n",
    "  Notice how the x value is the same in both `df_uk_greece_grouped['year']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_uk_greece_grouped = df_uk_greece.groupby(['year'],as_index=False).sum()\n",
    "# plot(x,y,fmt) - x is years, y is cases, \n",
    "# fmt is saying to use a dot marker, with no line connecting data\n",
    "# and the default colors for these markers\n",
    "plt.plot(df_uk_greece_grouped['year'],\n",
    "         df_uk_greece_grouped['cases_no_uk'],\n",
    "         '.')\n",
    "plt.plot(df_uk_greece_grouped['year'],\n",
    "         df_uk_greece_grouped['cases_no_greece'],\n",
    "         '.')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas DataFrames also have a `plot` method, which typically uses matplotlib behind the scenes. These can create nice smart plots directly from the dataframe. In some cases, this is the easier approach to use. For example, here is a bar plot of both groups over the years. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uk_greece_grouped.plot.bar(x='year',y=['cases_no_uk','cases_no_greece'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## 10. Transforming variables\n",
    "When we visually compare the number of cases in the UK with the number of cases in Greece, there seems to be a rather large difference between the two countries. Why would that be?\n",
    "\n",
    "The first thing I would check is whether there might be a relationship of cases with another variable in our data frame. Why don't we start by seeing what the populations between these two countries are?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_pop = df_uk_greece_grouped['population_uk'].mean()\n",
    "greece_pop = df_uk_greece_grouped['population_greece'].mean()\n",
    "print(\"Mean population over the years in this study:\")\n",
    "print(f\"UK: {int(uk_pop)}\")\n",
    "print(f\"Greece: {int(greece_pop)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't know about you, but I find all those numbers make it difficult to compare. Notice some small changes I do in this cell to make it a little easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_pop = np.round(uk_pop,decimals=-6)\n",
    "greece_pop = np.round(greece_pop,decimals=-6)\n",
    "print(\"Mean population over the years in this study:\")\n",
    "print(f\"UK:     {int(uk_pop)}\")\n",
    "print(f\"Greece: {int(greece_pop)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK! So if the UK has roughly 6 times larger population, then maybe they would have the 6 times more cases? Let's make a per capita variable and find out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uk_greece_grouped['cases_percap_uk'] = df_uk_greece_grouped['cases_no_uk'] / df_uk_greece_grouped['population_uk']\n",
    "df_uk_greece_grouped['cases_percap_greece'] = df_uk_greece_grouped['cases_no_greece'] / df_uk_greece_grouped['population_greece']\n",
    "df_uk_greece_grouped.plot.bar(x='year',y=['cases_percap_uk','cases_percap_greece'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the number of cases per capita in the UK is still higher than Greece, the difference between the two countries is much smaller than observed without taking population into effect.\n",
    "\n",
    "Here we have done a _transformation_ of the variable by normalizing it to another variable (in this case `population`), such that we are getting variables that are, more or less, on the same scale and we can compare between countries a bit better. \n",
    "\n",
    "Another transformation often used in data science is to normalize the data to make sure that the variable is scaled from 0 to 1, such that the maximum value observed was normalized to 1.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two steps to do this type of normalisation.\n",
    "\n",
    "**Step 1)** Create a function of normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    return (x)/x.max()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2)** Apply the function to the selected column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uk_greece_grouped['cases_norm_uk'] = normalize(df_uk_greece_grouped['cases_no_uk'])\n",
    "df_uk_greece_grouped['cases_norm_greece'] = normalize(df_uk_greece_grouped['cases_no_greece'])\n",
    "df_uk_greece_grouped.plot.bar(x='year',y=['cases_norm_uk','cases_norm_greece'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By normalizing the values, they are now on the same scale, but direct comparisons between countries become more difficult. This is because we have normalized these variables with respect to themselves and not accounting for the other country. All we can really do is compare the trends as to when more cases were observed in one country versus another. \n",
    "\n",
    "A similar form of normalisation, which you may have heard about or worked with, is to transform values to a standard distribution to obtain Z scores. This transforms the variable such that the mean of the variable is 0 and the standard deviation is 1.  \n",
    "\n",
    "We again start by creating a small helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore(x):\n",
    "    return (x-x.mean())/x.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uk_greece_grouped['cases_zscore_uk'] = zscore(df_uk_greece_grouped['cases_no_uk'])\n",
    "df_uk_greece_grouped['cases_zscore_greece'] = zscore(df_uk_greece_grouped['cases_no_greece'])\n",
    "df_uk_greece_grouped.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the z scored variables will have a different look from the normalized and per capita transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uk_greece_grouped.plot.bar(x='year',y=['cases_zscore_uk','cases_zscore_greece'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standardized scores over time doesn't allow for too much direct comparison between countries, but it does show general temporal trends within a country, where there were a higher number of cases in the late 80's and early 90s in the UK, and higher cases post-2011 in Greece. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. More exploration and visualisation\n",
    "Now that we have done some very initial plots, we can think about more advanced plots with the data. We may want to ask ourselves what is the relationship is between cases and population as well as cases and gdp. Is that relationship different between sexes?\n",
    "\n",
    "These sorts of questions are probably the realm of a more advanced statistical analysis, but we can start to get an idea of a relationship by doing. We will start by grouping the variables in a way that we can plot these differences.\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going back to the complete case data for this one. But we also\n",
    "# want to include sex in the grouping so that we can compare\n",
    "groupedby_country_yr_sex = df_complete.groupby(['country','year','sex'])\n",
    "# Sum up population and cases over country, year, and sex\n",
    "df_cases_plt = groupedby_country_yr_sex[['cases_no','population']].sum()\n",
    "# Just keep the original GDP, but put it on a scale of millions of USD\n",
    "df_cases_plt['gdp_millions_usd'] = groupedby_country_yr_sex[['gdp_for_year_usd']].first()/1.0e+6\n",
    "# Select just the males\n",
    "# Because the groupby function stores our groups as indices,\n",
    "# we need a helper function to find the right indices\n",
    "idx = pd.IndexSlice\n",
    "males = df_cases_plt.loc[idx[:,:,'male']]\n",
    "# Find all of the rows with females\n",
    "females = df_cases_plt.loc[idx[:,:,'female']]\n",
    "print(males.head())\n",
    "print(females.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can put multiple plots together using `matplotlib.pyplot.figure()` combined with `matplotlib.pyplot.add_subplot()`.  This method takes 3 parameters. The first denotes the total number of  rows of subplots, the second parameter denotes the total number of columns of subplots, and the final parameter denotes which subplot you wish to plot in (starting left-to-right then moving down a row). Once a subplot is created, the axes of these subplots can be decorated using methods like `set_title()` or `set_xlabel()` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig=plt.figure(figsize=(12.0, 8.0))\n",
    "# Plot one Cases versus population for males\n",
    "ax1= fig.add_subplot(2,2,1)\n",
    "# First plot - across all countries and yearsHere we plot bar using the plt.bar() function and it is set at the ax1 which is axes1 subplot variable\n",
    "ax1.set_title('Cases versus Population - Males') \n",
    "ax1.plot(males['population'],males['cases_no'],'.')\n",
    "\n",
    "# Plot two Cases versus GDP for males\n",
    "ax2= fig.add_subplot(2,2,2)\n",
    "ax2.set_title('Cases versus GDP - Males')\n",
    "ax2.plot(males['gdp_millions_usd'],males['cases_no'],'.')\n",
    "\n",
    "ax3= fig.add_subplot(2,2,3)\n",
    "ax3.set_title('Cases versus Population - Females') \n",
    "ax3.plot(females['population'],females['cases_no'],'.')\n",
    "\n",
    "ax4= fig.add_subplot(2,2,4)\n",
    "ax4.set_title('Cases versus GDP - Females')\n",
    "ax4.plot(females['gdp_millions_usd'],females['cases_no'],'.')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, these are more complex relationships than a simple linear plot, likely involving some geography, age, and the year of the sample. Population may also be affecting the GDP value as well. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Exercise 7\n",
    "Calculate the GDP/capita of the countries and visualise the cases according to GDP/capita as well as cased/capita versus GDP/capita for both males and females.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your answer for Exercise 7 here. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thank you for completing this lab!\n",
    "\n",
    "## Authors\n",
    "##  Mary Tziraki and David Cash\n",
    "\n",
    "\n",
    "                                                                                     \n",
    "\n",
    "## <h3 align=\"center\"> Â© Health+Bioscience IDEAS 2022. All rights reserved. <h3/>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "a0d4121080b5f33a2f2ddb6a1f46c0838d0d44af1b5f7e501e0b1477e85aebe3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
