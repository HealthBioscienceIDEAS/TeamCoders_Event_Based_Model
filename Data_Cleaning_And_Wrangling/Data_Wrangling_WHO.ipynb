{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a href=\"https://pandas.pydata.org/\">\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/ed/Pandas_logo.svg/2560px-Pandas_logo.svg.png\" width=\"300px\">\n",
    "</a>\n",
    "\n",
    "\n",
    "# Data Wrangling with pandas\n",
    "\n",
    "\n",
    "\n",
    "## Objectives\n",
    "This is the demonstration notebook for the first task. The facilitators will present a lecture that is based around this notebook. Before performing the first task as a team, we recommend you first go through this walkthrough together, as it will show a lot of techniques and code that will be useful in the task.  \n",
    "\n",
    "\n",
    "The main objective of this walkthrough is to familiarize you with the [pandas python package](https://pandas.pydata.org/). Pandas is a widely-used popular Python package that can discover, clean and structure tabular data. In particular, this notebook will cover:\n",
    "* The basic concept behind the core data structures within Pandas: [Series](https://pandas.pydata.org/docs/user_guide/dsintro.html#series) and [DataFrames](https://pandas.pydata.org/docs/user_guide/dsintro.html#dataframe)\n",
    "* How to process, organize and clean data stored in DataFrames\n",
    "* Visualization of data stored within a DataFrame.\n",
    "\n",
    "After completing this notebook you will be able to:\n",
    "* Understand key features of the Pandas library.\n",
    "* Import tabular data from spreadsheets into Pandas\n",
    "* Understand your data, its size, features and its structure.\n",
    "* Handle missing values\n",
    "* Correct data format\n",
    "* Transform variables, including grouping continuous values and normalization\n",
    "* Perform statistical analysis\n",
    "* Visualise data using Matplotlib library\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do we mean by data wrangling?\n",
    "Data wrangling is the process of reading in the raw data and then cleaning and processing it so that the resulting information is in suitable format for analysis. Part of the data wrangling process includes data discovery, data cleaning (identifying and then removing any erroneous data), handling missing data, transforming variables, and visualizing the resulting data.\n",
    "\n",
    "Data wrangling is often needed before any quantitative analysis can begin; however, it is often the most time-consuming and tedious part of the process. There are many different skills and actions that are required as part of data wrangling, many of which are illustrated in the example code below. Many of these steps are part of the functionality that Pandas provides. \n",
    "\n",
    "Here are some common, often used steps that are considered part of data wrangling:\n",
    "\n",
    "* **Discovering**: \n",
    "Reviewing and understanding the input data to better understand its structure and what variables will be useful for your problem \n",
    "* **Structuring**:\n",
    "Standardising the format for disparate types of data and make the data usable for automated or semiautomated data analysis. The data must be structured to fit the analytics model \n",
    "COME BACK TO THIS\n",
    "\n",
    "* **Cleaning** : \n",
    "There are often missing or implausible values in a dataset. These occur for a number of reasons and they could adversely affect the reliability and repeatability of an analysis. Cleaning techniques are identify and in some cases correct these instances so that they can be handled appropriately in subsequent analyses.\n",
    "\n",
    "* **Enriching**\n",
    "You are pretty familiar with the data at this point. This is the moment to ask yourself if you want to enhance the data. Are you looking to add other data to it?\n",
    "\n",
    "\n",
    "*  **Validating** \n",
    "This step involves iterative programming steps that authenticate your dataâ€™s quality and safety. For example, you may have problems if your data is not clean or enriched and the attributes are not distributed evenly.\n",
    "\n",
    "Many of these different stages of data wrangling require you to visualize the data in some form other than the base tabular data. Visualisation libraries such as matplotlib and seaborn are often used to plot data as well as perform some basic statistical analysis. For example, one simple way of identify outliers for cleaning is to plot the data points and visually identify points that appear to be well outside the plausible range of the rest of the data for a given variable. \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [1. Importing pandas and reading in data](#1-importing-pandas-and-reading-in-data)\n",
    "* [2. Discovering and reviewing the data](#2-discovering-and-reviewing-the-data)\n",
    " \n",
    "    <li><a href=\"#Identify_missing_Data\">Identify and handle missing values</a></li>     \n",
    "    <li><a href=\"#deal_missing_values\">Deal with missing values</li>\n",
    "    <li><a href=\"#correct_data_format\">Correct data format</a></li>\n",
    "    <li><a href=\"https://#data_standardization\">Data standardization</a></li>\n",
    "    <li><a href=\"https://#data_normalization\">Data normalization (centering/scaling)</a></li>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Importing pandas and reading in data\n",
    "### 1.1 Importing packages\n",
    "The first thing that we need to do is to import the Python packages we are going to need. This includes pandas, [numpy](https://numpy.org/) (which Pandas uses to store data in the two-dimensional tabular structure), and [matplotlib](https://matplotlib.org/), a popular Python package used for data visualisation.\n",
    "\n",
    "For convenience, we will import these packages with the widely agreed shortname (`pd`,`np`,`plt`), so that we don't always have to provide the package name each time we use some functionality from the package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.2 Reading in data\n",
    "For this notebook, we are using a dataset provided by the World Health Organisation. There are many sources where you might get data from that you will then want to work with using pandas. The most common is to open a file containing tabular data. The most well-known format for tabular data are Excel spreadsheets. Another commonly used format are plain-text *comma separated values* files with the extension `csv`. Each line of a csv file represents a row, and each column is separated by a comma. \n",
    "\n",
    "Besides local files, you may also get data by querying a database or requesting data from the web. \n",
    "\n",
    "All of these sources generate tabular data that can be stored within pandas DataFrames.\n",
    "\n",
    "For the purpose of this notebook, we have assumed you have [downloaded the data](https://liveuclac.sharepoint.com/:x:/r/sites/TeamCodersEventsPlanning/Shared%20Documents/General/WHO/who_case_statistics_modified3.csv?d=wec3e29c1cb9d43779f37cc43bf9eed3b&csf=1&web=1&e=wy7e84) and stored it to a local file in the same directory as this notebook. \n",
    "\n",
    "pandas has many different functions that will read in data depending on the data format. Below, we will read the filename and store it the variable `input_file` and then use the [read_csv](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) function to read this file \n",
    "and store it into a DataFrame, under a variable named `df_raw`. Here the df stands for DataFrame, so that you know what is stored in the variable.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If you have not downloaded the file into this directory\n",
    "# please ammend the first line so that input_file contains\n",
    "# the full location (path) of the file\n",
    "input_file = \"who_case_statistics_modified3.csv\" \n",
    "df_raw = pd.read_csv(input_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Discovering and reviewing the data \n",
    "Before we can work with the data, we need to know about more about the contents of the data. Here are some core functions that can help you start to review the structure and contents of your DataFrame. \n",
    "* `pandas.DataFrame.head()` - prints out the first *n* rows of the DataFrame \n",
    "* `pandas.DataFrame.tail()` - prints out the last *n* rows of the DataFrame\n",
    "\n",
    "You can understand a bit more of the data structure using the following commands:\n",
    "* `pandas.DataFrame.info()` - summary information (column names, counts, data types) \n",
    "* `pandas.DataFrame.describe()` - provides basic summary statistics for each variable\n",
    "\n",
    "Some basic low-level information about the DataFrame can be extracted with the following attributes:\n",
    "* `pandas.DataFrame.shape` - provides the number of rows and columns in the DataFrame\n",
    "* `pandas.DataFrame.dtypes` - indicates the datatype (string, float, integer, etc) used to store the data for each variable\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Review the data\n",
    "Use the method <b>head()</b> to display the first five rows of the dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>year</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>cases_no</th>\n",
       "      <th>population</th>\n",
       "      <th>HDI for year</th>\n",
       "      <th>gdp_for_year ($)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Albania</td>\n",
       "      <td>1987</td>\n",
       "      <td>male</td>\n",
       "      <td>15-24 years</td>\n",
       "      <td>21</td>\n",
       "      <td>312900</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2,156,624,900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Albania</td>\n",
       "      <td>1987</td>\n",
       "      <td>male</td>\n",
       "      <td>35-54 years</td>\n",
       "      <td>16</td>\n",
       "      <td>308000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2,156,624,900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Albania</td>\n",
       "      <td>1987</td>\n",
       "      <td>female</td>\n",
       "      <td>15-24 years</td>\n",
       "      <td>14</td>\n",
       "      <td>289700</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2,156,624,900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Albania</td>\n",
       "      <td>1987</td>\n",
       "      <td>male</td>\n",
       "      <td>75+ years</td>\n",
       "      <td>1</td>\n",
       "      <td>21800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2,156,624,900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Albania</td>\n",
       "      <td>1987</td>\n",
       "      <td>male</td>\n",
       "      <td>25-34 years</td>\n",
       "      <td>9</td>\n",
       "      <td>274300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2,156,624,900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   country  year     sex          age cases_no  population  HDI for year  \\\n",
       "0  Albania  1987    male  15-24 years       21      312900           NaN   \n",
       "1  Albania  1987    male  35-54 years       16      308000           NaN   \n",
       "2  Albania  1987  female  15-24 years       14      289700           NaN   \n",
       "3  Albania  1987    male    75+ years        1       21800           NaN   \n",
       "4  Albania  1987    male  25-34 years        9      274300           NaN   \n",
       "\n",
       "   gdp_for_year ($)   \n",
       "0      2,156,624,900  \n",
       "1      2,156,624,900  \n",
       "2      2,156,624,900  \n",
       "3      2,156,624,900  \n",
       "4      2,156,624,900  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27841, 8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 27840 entries, 0 to 27839\n",
      "Data columns (total 8 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   country             27840 non-null  object \n",
      " 1   year                27840 non-null  int64  \n",
      " 2   sex                 27840 non-null  object \n",
      " 3   age                 27840 non-null  object \n",
      " 4   cases_no            23575 non-null  object \n",
      " 5   population          27840 non-null  int64  \n",
      " 6   HDI for year        8368 non-null   float64\n",
      " 7    gdp_for_year ($)   27840 non-null  object \n",
      "dtypes: float64(1), int64(2), object(5)\n",
      "memory usage: 1.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df_raw.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "Use the tail method to see the last part of the data.\n",
    "How many lines do you see?\n",
    "Use the the tail method again to see the last 20 lines of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "Use the info method to gather information  about the data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "* The data frame contains 27841 rows and 8 columns.\n",
    "* The data frame is an object of class pandas.DataFrame.\n",
    "* Columns that hold only numeric data are stored as floats or integers\n",
    "* Columns that hold text or a mixture of data are stored as objects. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "Write any other observation that you see about the data. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Understanding headers\n",
    "How did pandas know how to name the columns? If you opened up the file in a text editor, you would see that the first row contains these names. When you run `read_csv`, pandas assumes that the column names (or _headers_) are located in the first row. If that is not the case, you can specify which row they can be found by using the `headers` argument. \n",
    "\n",
    "However, if there are not any column names in the file, then it is important to tell pandas that, lest you lose actual data. This can be done by setting `headers` to `None`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the data - but assuming no headers\n",
    "df_raw = pd.read_csv(input_file, headers=None)\n",
    "# Now look at the head, does it look correct?\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also look at the info, does it look correct?\n",
    "df_raw.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Selecting the columns and getting the values\n",
    "The way to to select columns is to write the \n",
    "`dataframe['column_name']`\n",
    "Lets show all the countries in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0            Albania\n",
      "1            Albania\n",
      "2            Albania\n",
      "3            Albania\n",
      "4            Albania\n",
      "            ...     \n",
      "27835        Belgium\n",
      "27836       Thailand\n",
      "27837    Netherlands\n",
      "27838        Grenada\n",
      "27839         Mexico\n",
      "Name: country, Length: 27840, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_raw = pd.read_csv(input_file)\n",
    "countries=df_raw['country']\n",
    "print(countries)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the counts of different values in the countries column using the `pandas.DataFrame.value_counts()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Austria                   383\n",
       "Netherlands               383\n",
       "Iceland                   382\n",
       "Mauritius                 382\n",
       "Belgium                   373\n",
       "                         ... \n",
       "Bosnia and Herzegovina     24\n",
       "Macau                      12\n",
       "Cabo Verde                 12\n",
       "Dominica                   12\n",
       "Mongolia                   10\n",
       "Name: country, Length: 101, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw['country'].value_counts() #select the column with data_raw['country'] then add the method .value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows us how many times each country is found in the column. If you wanted instead to get a list of all countries found in this column, you can use the `unique` function. The output from this will be a one-dimensional array.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Albania', 'Antigua and Barbuda', 'Argentina', 'Armenia', 'Aruba',\n",
       "       'Australia', 'Austria', 'Azerbaijan', 'Bahamas', 'Bahrain',\n",
       "       'Barbados', 'Belarus', 'Belgium', 'Belize',\n",
       "       'Bosnia and Herzegovina', 'Brazil', 'Bulgaria', 'Cabo Verde',\n",
       "       'Canada', 'Chile', 'Colombia', 'Costa Rica', 'Croatia', 'Cuba',\n",
       "       'Cyprus', 'Czech Republic', 'Denmark', 'Dominica', 'Ecuador',\n",
       "       'El Salvador', 'Estonia', 'Fiji', 'Finland', 'France', 'Georgia',\n",
       "       'Germany', 'Greece', 'Grenada', 'Guatemala', 'Guyana', 'Hungary',\n",
       "       'Iceland', 'Ireland', 'Israel', 'Italy', 'Jamaica', 'Japan',\n",
       "       'Kazakhstan', 'Kiribati', 'Kuwait', 'Kyrgyzstan', 'Latvia',\n",
       "       'Lithuania', 'Luxembourg', 'Macau', 'Maldives', 'Malta',\n",
       "       'Mauritius', 'Mexico', 'Mongolia', 'Montenegro', 'Netherlands',\n",
       "       'New Zealand', 'Nicaragua', 'Norway', 'Oman', 'Panama', 'Paraguay',\n",
       "       'Philippines', 'Poland', 'Portugal', 'Puerto Rico', 'Qatar',\n",
       "       'Republic of Korea', 'Romania', 'Russian Federation',\n",
       "       'Saint Kitts and Nevis', 'Saint Lucia',\n",
       "       'Saint Vincent and Grenadines', 'San Marino', 'Serbia',\n",
       "       'Seychelles', 'Singapore', 'Slovakia', 'Slovenia', 'South Africa',\n",
       "       'Spain', 'Sri Lanka', 'Suriname', 'Sweden', 'Switzerland',\n",
       "       'Thailand', 'Trinidad and Tobago', 'Turkey', 'Turkmenistan',\n",
       "       'Ukraine', 'United Arab Emirates', 'United Kingdom',\n",
       "       'United States', 'Uruguay', 'Uzbekistan'], dtype='<U28')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_array = df_raw['country'].unique()\n",
    "country_list = df_raw['country'].unique().astype('str')\n",
    "country_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data_raw_countries_alphabetically)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data_raw_countries_alphabeticallly is a numpy array that we need to convert it to list, using the **.tolist()** method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_countries_list= data_raw_countries_alphabetically.tolist() # Convert a numpy array to a list\n",
    "data_countries_list  # The list starts and finishes with []\n",
    "#  we can print the list within the print function:      print(f'The list of countries is:{data_countries_list}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the length of the list\n",
    "len(data_countries_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data columns Investigation \n",
    "### Find the mean of a numerical column\n",
    "The method to find the mean of a column is **.mean()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw_suicides = data_raw['suicides_no']\n",
    "data_raw_suicides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw_suicides_mean = data_raw['suicides_no'].mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ERROR !\n",
    " We observe that:\n",
    "There are  string values at the suicide_no column and we cannot find the mean with the .mean() method because it applies only to integers or strings!\n",
    "\n",
    "Lets to display all the unique values in the column of 'suicides_no' with the method **.unique()** and change the type of these values to integers using typecasting **.astype(int)**.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw_suicides = data_raw['suicides_no'].unique().astype(int) # Convert the data into the suicides_no column into an integer\n",
    "data_raw_suicides"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error and Observations:\n",
    "   * There are many NaN (Not a Number) data points in the suicide_no column, that indicate missing values and these cannot converted into integer.\n",
    "   * As displayed at the data_raw above the HDI column is dominated by NaN. We need to delete the column with the Human Development Index because we will not use it for the analysis and it has lots of missing values that we cannot deal with them. We will also save space and time. \n",
    "   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 3) **How to work with missing data?**\n",
    "As we can see, there are several NaN (not a number) in the data frame, it might also be word  Null or any other words and string values. Those are missing values which may hinder our further analysis. I'm showing here, how do we identify all those missing values and work with missing data in order to bring our Data Frame at a format that is ready for analysis.\n",
    "\n",
    "Steps for working with missing data:\n",
    "\n",
    "</ol>\n",
    "    <li>Identify missing data</li>\n",
    "    <li>Deal with missing data</li>\n",
    "    <li>Correct data format</li>\n",
    "</ol>\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2 id=\"Identify_missing_Data\">3.1 Identify missing Data</h2>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **3.1.1 Find missing data**\n",
    "The missing values are converted by default to Null. We use the following methodsto identify these missing values:\n",
    "\n",
    "- **.isnull()**\n",
    "- **.notnull()**\n",
    "The output is a boolean value indicating whether the value that is passed into the argument is in fact missing data.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we use the **isnull()** method the output is True when there is Null, and False when there is a real value. The opposite happens when we use the .notnull() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(data_raw.isnull()).value_counts() # Indicates with True that there are Nulls and counts the values that are numbers "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are Null values in just two columns at the suicides_no and at the HDI for year column"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The opposite we observe with **.notnull()** Similar to not null is to use the **(~)** in front of the dataset and then the .isnull(). The tilde sign **(~)** indicates negation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(data_raw.notnull()).value_counts() # Indicates with True that we have values and with False where there are Nulls and counts the values that are numbers (not Null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(~data_raw.isnull()).value_counts() # The ~ indicates negation and here in front of isnull results to notnull()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can have the same result but reverted Booleans by using the **.notnull()** method.\n",
    "\n",
    "**(~data_raw.isnull()).value_counts()** # The tilde symbol indicates negation and here in front of isnull results to notnull()\n",
    "\n",
    "**(data_raw.notnull()).value_counts()** # Indicates with False that there are Nulls and counts the values that are numbers \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data = data_raw.isnull() # find data that are null and return 'True' if is null and 'False' if it is not\n",
    "missing_data.head(20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.1.2.Count the missing values in the data frame**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count missing values in each column\n",
    "\n",
    "Using a for loop in Python, we can quickly figure out the number of missing values in each column. As mentioned above, \"True\" represents a missing value and \"False\" means the value is present in the dataset.  In the body of the for loop the method **\".value_counts()\"** counts the number of data values, (number of  \"False\" here). \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data=(data_raw.isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(missing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The for loop \n",
    "for column in data_raw.columns: # Select each column-name in the header and prints it \n",
    "    print(column)\n",
    "    print(missing_data[column].value_counts())\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#The commented code give the same result as the one that follows but is shortened as puts the methods together.\n",
    "#for column in missing_data.columns.values.tolist():  # Take the data Frame missing_data,selects the columns and their values and makes a list of them.  \n",
    " #print(column)\n",
    " #print (missing_data[column].value_counts())\n",
    " ## print(\"\") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An easier way is to show the data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data = data_raw.isnull() # find data that are null and return 'True' if is null and 'False' if it is not\n",
    "missing_data.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed the only columns that contain missing data are the \"suicides_no'and the HDI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (data_raw.isnull().sum()) # Find the data that are Null in the whole data frame  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The length of the dataframe is 27841 and the column suicides_no has 23575 numbers. It is shorter than the total length of the data 27841 by 27841- 23575= 4265. It is indicated as True (is null) in the count. The missing values therefore there must be strings characters in the column. We need to investigate it.\n",
    "\n",
    "The HDI column has lots of missing data 19472 values are missing. We will not use it for the analysis and it will be good to delete this column."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3.2 Investigation for alpharithmetic characters or words misplaced as values**\n",
    "There are two ways to identify the non-integer values in the column suicides_no\n",
    "1) Using the module Regular expressions which we need to import as re. \n",
    "A regular expression is a special sequence of characters that helps you match or find other strings or sets of strings, using a specialized syntax held in a pattern.\n",
    "\n",
    "2) Find the non numeric data with the method **.str.isdigit()** and set it to False that way it collects all the data points that are strings."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st Way Regular expressions module **re**\n",
    "\n",
    "If you want to run the following code uncomment it from the import downwards.\n",
    "As I have this notebook run I have selected the 2nd method (.str.isdigit())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import module Regular expressions as re.\n",
    "#The re module provides an interface to the regular expression engine, \n",
    "# allowing you to compile REs into objects and then perform matches with them\n",
    "\n",
    "\n",
    "#import re \n",
    "#replace = re.compile(\"([a-zA-Z]+)\")  # compile any alphabetic character\n",
    "\n",
    "#data_raw['string'] = data_raw['suicides_no'].str.extract(replace) # Adds a column string to the data set with all the strings which have letters\n",
    "#data_raw['integer'] = data_raw['suicides_no'].str.replace(replace, \" \") #Adds a column integer to the data set \n",
    "\n",
    "#data_raw['string'].unique() # Presents the unique \"words\" from the string column"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In the column suicide-no there are the strings **'Null'**, **'Unknown'** and **nan (not a number)** and we need to replace them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd Way without is to use the **.isdigit()** method to a string.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code takes all the strings in the column which have digits and sets it to False. Which means that takes all the strings that have no digits. Using the method **unique()** identifies which are these strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_numeric=data_raw['suicides_no'].str.isdigit() == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type (non_numeric)# The non_numeric is 1D array, Series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw['suicides_no'][non_numeric].unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that there are not only the NaN strings but there are also the \"Null\" and \"Unknown\" strings and we need to remove them"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot see the NaN values and we apply the pd.numeric (explain)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's an elegant way to find the strings and words in the datasets without the regular expresions, however it doesn't show us the NaN datapoints. We can add this with the following coding which is more complete and shorten than regular expresions (re)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suicides =data_raw['suicides_no']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suicides2 = suicides.apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_raw['suicides_no'][suicides2.isna()].unique())# Shows the data which are not numeric"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above shows that in the data there are points which as NaN (not an number) and have the word Null and Unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_null=data_raw.loc[data_raw['suicides_no']=='Null']\n",
    "df_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw.head(15)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the column suicide-no there are the strings 'Null', 'Unknown' and NaN and we need to replace them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"deal_missing_values\">4 Deal with missing data</h2>\n",
    "<b>How to deal with missing data?</b>\n",
    "\n",
    "<ol>\n",
    "    <li>Drop data<br>\n",
    "        a. Drop the whole row<br>\n",
    "        b. Drop the whole column\n",
    "    </li>\n",
    "    <li>Replace data<br>\n",
    "        a. Replace it by mean<br>\n",
    "        b. Replace it by frequency<br>\n",
    "        c. Replace it based on other functions\n",
    "    </li>\n",
    "</ol>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1  Drop / Delete a column with lots of missing data.\n",
    "To remove a column we use the .drop() method as **df.drop([\"column1, column2, ... \"], axis=1)**. the axis =1 indicates that I apply the method to columns.\n",
    " * I will cut the column HDI for year, There are a lot of missing values there and I don't think that if contributes to the result.\n",
    " \n",
    " * I will name the new Data Frame **data_frame** because it is not the raw data anymore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you have run the Regular expressions and you have created the columns 'string' and 'integers', please run the code I have commented. \n",
    "#data_frame= data_raw.drop(['HDI for year','string','integer'], axis = 1) \n",
    "\n",
    "# If  you have run the 2nd way to identify the string values, which is with the .isdigit(), run the following command. \n",
    "data_frame= data_raw.drop(['HDI for year'], axis = 1)                         \n",
    "data_frame"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have investigated that there are 4265 missing integers in the suicide_no column which would be the strings \n",
    "'NaN', 'Unknown', \"Null\".\n",
    "\n",
    "We have deleted the HDI for year colum because it had a lot of missing values\n",
    "\n",
    "We have created a new data_frame with the data, which is not longer the raw data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  4.2 REPLACE :\n",
    "#### Locate the data in suicides_no that appeared as strings and REPLACE them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_null=data_frame.loc[data_frame['suicides_no']=='Null']\n",
    "df_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_null.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 4265 points at the suicides_no with the wrong/missing values. It's exactly what we have calculated with the missing_values variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Count the values of the 'Null' in 'suicides_no' column \n",
    "#data_null[\"suicides_no\"].value_counts()  # Same as:  data_null.suicides_no.value_counts()  \n",
    "df_null.suicides_no.value_counts()  \n",
    "\n",
    "### If we want to be concise we could write it elegantly in one line\n",
    "# data_frame.loc[data_frame['suicides_no']=='Null'].suicides_no.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unknown=data_frame.loc[data_frame['suicides_no'] =='Unknown'] \n",
    "df_unknown  ## show the lines with the word Unknown in the suicides_no column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unknown.suicides_no.value_counts() ## count the values with the word Unknown in the suicides_no column "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REPLACE  incorrect / missing values!\n",
    "There are three types of data in the column 'suicides_no' in our Data Frame that need to be cleared out. The word 'Null',the word \"Unknown\" and NaN (Not a Number). I will replace the word Null' with 0 , because its is actually zero. \n",
    "\n",
    "The strategy with other words which don't actually contribute to data (unless it is categorical data), is to replace them with the string NaN.  I replace \" \" with NaN (Not a Number), which is Python's default missing value marker, for reasons of computational speed and convenience. Then we can delete all the NaN data points.\n",
    "- Replace all strings with NaN\n",
    "- Delete all NaN values\n",
    "\n",
    "Here we use the functions to use to localise and replace one value with another:\n",
    "\n",
    "* .loc[data['Column Name']=='Character to localise']\n",
    "to localise the Characters in the Column name\n",
    "* .replace(A, B, inplace = True) \n",
    "to replace A value  by B value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace an empty space \" \" and \"Unknown\" to NaN and the \"Null\" with 0 using Numpy library!!\n",
    "data_frame.replace(\" \", np.nan, inplace = True)\n",
    "data_frame.replace(\"Null\", 0, inplace = True)\n",
    "data_frame.replace(\"Unknown\", np.nan, inplace = True)\n",
    "data_frame.head(15)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets count now how many columns are NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_NaN=data_frame.loc[data_frame['suicides_no'] =='NaN'] \n",
    "#df_NaN.suicides_no.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the summary above, each column has 27840 rows of data and only one of the columns the suicide_no containes missing data:\n",
    "\n",
    "<ol>\n",
    "    <li>\"suicides_no\": 4273 missing data which are replaced with NaN</li>\n",
    "    \n",
    "</ol>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DELETE/DROP  NaN \n",
    "In order to deal with NaN we have to create a numpy array where the NaN appears. We can visualise the data frame by calling .head() or .tail()\n",
    "\n",
    "The way to  delete the NaN is  using the *.dropna()*  to the  data. Immediately afterwards you need to  to reset the index of the data frame as there are deleted values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_frame.tail(10).reset_index()  # use .reset_index()  to visualise it and count it better"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove the NaN values \n",
    "We can see the the entry points 27824, 27825, 27828  have NaN values at the suicides_no column. We use **.dropna()**. Always reset index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I will cut any data points with NaN since I don't know with what to replace them\n",
    "data_frame = data_frame.dropna() # deletes the rows with NaN\n",
    "data_frame.tail(10).reset_index()  ## reset the values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whole columns or rows should be dropped only if most entries in the column are empty. In our dataset, none of the columns are empty.\n",
    "\n",
    "\n",
    "There is some freedom in choosing which method to replace data; however, some methods may seem more reasonable than others. We will apply each method according to the data and what the column represents :\n",
    "\n",
    "**Replace by mean:** depending on the concept the  missing data could be replaced them with mean\n",
    "   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the mean value for the \"suicides_no\" column\n",
    "It does not make any sense with this data but I display how to substitute with the mean to show the method. \n",
    "If you want to use it elsewhere you can remove the # in front of the code and use them \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to calculate and show the average of a column\n",
    "avg_suicide_no=data_frame['suicides_no'].astype('int').mean(axis=0)# First we need to make the values floats (type casting)\n",
    "print(\"Average suicides:\",avg_suicide_no )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Average suicides integer:\", int(avg_suicide_no))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace \"NaN\" with the integer average value in the \"suicide_no\" column.\n",
    "As written above we will not replace by a mean in this example.\n",
    "I just show the code to use it when appropriate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data_frame['suicides_no'].replace(np.nan, avg_suicide_no, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace by Frequency:\n",
    "Find the most frequent value ( the mode) of the column and replace the NaN with the most frequent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_frame['suicides_no'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 1576 different numbers in the column.  We can also use the \".idxmax()\" method to calculate the most common value automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame['suicides_no'].value_counts().idxmax()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that number **'1' is the most common value**.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The replacement procedure is very similar to what we have seen previously:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace the missing 'NaN' values in the suicides_no column  by the most frequent number which is 1 \n",
    "#data_frame['suicides_no'].replace(np.nan, 1 , inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Good!</b> Now, we have a dataset with no missing values.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"correct_data_format\"> 5 Correct data format </h2>\n",
    "\n",
    "We are almost there!\n",
    "The last step in data cleaning is checking and making sure that all data is in the correct format (int, float, text or other)\n",
    "\n",
    "In Pandas, we use:\n",
    "\n",
    "**.dtype()** to check the data type\n",
    "**.astype()**  to change the data type\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Let's list the data types for each column</h4>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, some columns are not of the correct data type. Numerical variables should have type 'float' or 'int', and variables with strings such as categories should have type 'object'. For example, 'year', 'age', 'suicides_no, 'population, and 'gdp_for_year ($)' variables are numerical values, so we should expect them to be of the type 'float' or 'int'; however, they are shown as type 'object'. We have to convert data types into a proper format for each column using the **.astype()** method."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Convert data types to proper format</h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame[[\"year\", \"suicides_no\", \"population\"]] = data_frame[[\"year\", \"suicides_no\", \"population\"]].astype(\"int\")\n",
    "#Type casting using .astype(\"float\")   .astype(\"str\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The problems of the column 'gdp_for_year '\n",
    "* The header of the column has a space before the dolar symbol 'gdp_for_year ($)' and we need to replace it with a   name that is coherent and like 'gdp_for_year_usd'\n",
    "* The format of the numbers in the column should be without the commas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st Change the header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers= data_frame.columns # Select the headers\n",
    "headers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column with the GDP has a header that contains empty spaces and the $ character, these will cause problems whenever we mention the header. We need to change it to a title that would be easy to write and avoid mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##There are spaces at the name of the column \"  gdp_for_year-usd  \" and I'm replacing it with a name without spaces \"gdp_for_year_usd\"\n",
    "## #Create a list (headers) and replace with the **lamda* method one \n",
    "headers = list(map(lambda x: x.replace(' gdp_for_year ($) ', 'gdp_for_year_usd'), headers)) #Create a list and replace with the **lamda* method one value of the list\n",
    "headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.columns=headers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd Change the format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_frame.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame['gdp_for_year_usd'] # Show the values, which are of the type object"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to REMOVE the COMMAS ' on the numbers and typecasting them to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First replace the string comma ',' empty space''. Then typecasting the column to integers\n",
    "data_frame['gdp_for_year_usd']= data_frame['gdp_for_year_usd'].str.replace(',', '').astype(int)\n",
    "data_frame['gdp_for_year_usd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here the gdp_for_year_usd column is integer and I dont need to run this cell but in case it is not we need to do the typecasting and set it to integer\n",
    "#data_frame['gdp_for_year_usd'].astype(\"int\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Let us list the columns after the conversion</h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_frame.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Wonderful!**\n",
    "#### Save the data\n",
    "\n",
    "Now we have finally obtained the cleaned dataset with no missing values with all data in its proper format.\n",
    "We can save this data set as it is now the 'clean' data set which is ready for processing. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.to_csv(r'./data/who_suicide_statistics_clean_data.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Operations**\n",
    "Data operations can be performed through various built-in methods for faster data processing and analysis. A few methods are:\n",
    "- Operations using Grouping\n",
    "- Operations with Sorting\n",
    "- Operations with Statistics \n",
    "- Operations with Functions (Standarisation and Normalisation)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Grouping\n",
    " Use  **.groupby('..')** Its a function of grouping the data. It involes a combination of **spliting** the object applying a function and **combine** the results **.groupby('column name, value')**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To select the data of a single country from the 101, I'll  group the data frame by 'country' and from that I will select the country of preference by using **.get_group('selection').\n",
    "Lets select the data for the United Kingdom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_grouped= data_frame.groupby(['country'])\n",
    "UK_data=country_grouped.get_group('United Kingdom')\n",
    "UK_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make another data frame by selecting the columns that we want to analyse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>year</th>\n",
       "      <th>suicides_no</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26476</th>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>1985</td>\n",
       "      <td>264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26477</th>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>1985</td>\n",
       "      <td>915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26478</th>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>1985</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26479</th>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>1985</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26480</th>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>1985</td>\n",
       "      <td>678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26843</th>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>2015</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26844</th>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>2015</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26845</th>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>2015</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26846</th>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>2015</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26847</th>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>2015</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>369 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              country  year  suicides_no\n",
       "26476  United Kingdom  1985          264\n",
       "26477  United Kingdom  1985          915\n",
       "26478  United Kingdom  1985          128\n",
       "26479  United Kingdom  1985           62\n",
       "26480  United Kingdom  1985          678\n",
       "...               ...   ...          ...\n",
       "26843  United Kingdom  2015          181\n",
       "26844  United Kingdom  2015           18\n",
       "26845  United Kingdom  2015           14\n",
       "26846  United Kingdom  2015            6\n",
       "26847  United Kingdom  2015            4\n",
       "\n",
       "[369 rows x 3 columns]"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UK_data_year_suicides= UK_data[['country','year', 'suicides_no']] # UK_data = country_grouped.get_group('United Kingdom'). If you want you could add another column as well\n",
    "UK_data_year_suicides"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the total suicides of each year, group two columns together and use the  method **.sum()**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UK_suicides_per_year =UK_data_year_suicides.groupby(['country','year'],as_index=False).sum()\n",
    "UK_suicides_per_year.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting values \n",
    "To sort values either according to their size or alphabetical order we use  the method **.sort_values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UK_suicides_per_year_sorted=UK_suicides_per_year.sort_values('suicides_no') # The default is ascending order\n",
    "UK_suicides_per_year_sorted"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations with Statistics \n",
    "The methods **.max()**, **.min()**,**.mean()**,**.std()** used to find the maximum, minimum, mean and standard deviation at data. The **.describe** function shows all the statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UK_suicides_per_year['suicides_no'].max() # Select from the data frame, the column that we are interested and apply the .max() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the print() function to print the numbers that you calculate with the .max(), .min(). mean() .std() and all the statistical methods\n",
    "print('The maximum number of cases in the UK is', UK_suicides_per_year['suicides_no'].max())\n",
    "print('The minimum number of cases in the UK is',UK_data_year_suicides['suicides_no'].min())\n",
    "print('The standard deviation of cases in the UK is', UK_suicides_per_year['suicides_no'].std())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate another country's data and create another sub-data \n",
    "\n",
    "Let's repeat what we have calculated and presented for the UK for another country. You could choose any country from the list of the initial data frame. I'm choosing Greece here and I repeat the code above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Investigate another data set \n",
    "country_grouped= data_frame.groupby(['country'])\n",
    "Greece_data=country_grouped.get_group('Greece')\n",
    "Greece_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Greece_data_year_suicides= Greece_data[['country','year', 'suicides_no']] # country_grouped.get_group('Greece').\n",
    "Greece_data_year_suicides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Greece_suicides_per_year =Greece_data_year_suicides.groupby(['country','year'],as_index=False).sum()\n",
    "Greece_suicides_per_year.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Greece_suicides_per_year_sorted=Greece_suicides_per_year.sort_values('suicides_no') # The default is ascending order\n",
    "Greece_suicides_per_year_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The maximum number of cases in Greece is', Greece_suicides_per_year['suicides_no'].max())\n",
    "print('The minimum number of cases in Greece is',Greece_data_year_suicides['suicides_no'].min())\n",
    "print('The standard deviation of cases in Greece',Greece_suicides_per_year['suicides_no'].std())\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine / Merge the data sets \n",
    "\n",
    "Another useful operation is to combine together data sets. Pandas is a powerful library and gives a multifaced approach to combining separate datasets. \n",
    "With pandas, you can **merge** , **join** and **concatenate** your datasets, allowing you to unify them , understand and interpret them. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![merge](./fig/fig_merge.jpg)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you use the **merge() function** , the default is an inner join, which in most of the cases result in a DataFrames which are shorter. With merge(), you also have control over which column(s) to join on. Letâ€™s say that you want to merge both entire datasets, but only on 'year'  since the combination of the two will yield a unique value for each row. To do so, you can use the on parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UK_suicides_per_year.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Greece_suicides_per_year.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UK_Greece_merged=pd.merge(UK_suicides_per_year, Greece_suicides_per_year,on='year') # Merges the two datasets (one next to the other) and keep common column the 'year' column\n",
    "UK_Greece_merged"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the datasets you can join on 'left' or on the 'right but in this example it doesn't mean anything: The indication is with the **how** parameter like below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_merge=pd.merge(UK_suicides_per_year, Greece_suicides_per_year,how='right', on='year')\n",
    "df1_merge "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the .join() method : Combine Data on a Column or index\n",
    "The merge () is a module function (which means that you write the DataFrames as arguments). The .join() is method.  This enables you to specify only one DataFrame, which will join the DataFrame you call .join() on. \n",
    "\n",
    "Here I'll create on data frame (Two_countries_combined) which I'll assign it  the Greek results and then I'll join to it the UK DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_country= Greece_suicides_per_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "two_countries_combined=one_country.join(UK_suicides_per_year.set_index([\"year\"]), on=[\"year\"], how= \"inner\", lsuffix=\"_x\", rsuffix=\"_y\") # It is important to set the index at th common column. The lsuffix sets the first data Frame on the left\n",
    "two_countries_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_countries_combined2=one_country.join(UK_suicides_per_year.set_index([\"year\"]), on=[\"year\"], lsuffix=\"_left\") # In this datasets if you dont use how \"inner\" it doesnt matter \n",
    "two_countries_combined2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the concat() method : Combine data Across Rows or Columns \n",
    "Concatenation is a bit different from the merging techniques with merge() function and .join() method where you would specify the merging (inner or outer), and depending on the type of merge you might lose rows and information. \n",
    "\n",
    "With the concatenation the  datasets are stitched together along wither the row axis or the column axis.\n",
    "\n",
    "If you us the following code that concatenates the dataframes (df1 and df2)  with no parameters the default results will look like the picture:\n",
    "\n",
    "**df_concatenated = pandas.concat([df1, df2])**\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![concatenate1](./fig/fig1_concat.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example assumes that your column names are the same.  If your column names are different while concatenating along rows (axis 0), then by default the columns will also be added, and NaN values will be filled in as applicable."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![concatenate2](./fig/fig2_concat.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to accomplish concatenation along columns instead, youâ€™ll use a concat() call like you did above, but youâ€™ll also need to pass the axis parameter with a value of 1 or \"columns\":\n",
    "\n",
    "**df_concatenated = pandas.concat([df1, df2], axis='columns\")**  or **df_concatenated = pandas.concat([df1, df2], axis=1)**\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![concatenate3](./fig/fig3_concat.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets concatenate our datasets and view them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_concatenated=pd.concat([UK_suicides_per_year, Greece_suicides_per_year])\n",
    "df1_concatenated"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This results to the datasets being one underneath the other. They are concatenated by default on the \"row axis\" or axis=0. If you look carefully the indexes are different for each data sets (start from 0-30 for the first dataframe and from 0-30 for the second one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_concatenated=pd.concat([UK_suicides_per_year, Greece_suicides_per_year], axis =1)\n",
    "df2_concatenated"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see the data frame 1 next to the other.\n",
    "\n",
    "By default, a concatenation results in a **set union**, where all data is preserved. Youâ€™ve seen this with merge() and .join() as an outer join, and you can specify this with the join parameter.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Visualise the data**\n",
    "\n",
    "Where you are investicating your data of prepare to publish your findings, visualisation is an essential tool. There are various libraries that you can use in Python to visualise your data most popular are Matplotlib, and Seaborn. Here we use the Matplotlib library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  **%matplotlib** is a magic function, if you use it just it displays the figure as a pup up. Using with inline, the figure is inside the noteboook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_merged=pd.merge(UK_suicides_per_year, Greece_suicides_per_year,how='right', on='year') #\n",
    "df1_merged \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can change the column names to be more specific or use the .join() method instead which have arguments lsuffix and rsuffix to specify it. \n",
    "Lets change the header as we have seen before at paragraph 1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers=['UK','year','suicides_no_UK','Greece','suicides_no_Greece']\n",
    "df1_merged.columns=headers\n",
    "df1_merged.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to plot. Firts import the pyplot module of the matplotlib library as plt.\n",
    "then use the .plot() function at our dataframe. The arguments start with x column and y column/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "df1_merged.plot(x='year', y=['suicides_no_UK','suicides_no_Greece'])\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default plot is line plot. There are different other kinds of plots and we need to specify them in the arguments these are : bar, area, barh=horizontal bars, hist= histogramms, box, pie= piecharts, line, scatter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_merged.plot(kind= 'bar', x='year', y=['suicides_no_UK','suicides_no_Greece'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations with Functions Statistical Functions\n",
    "### Example Statistical Functions of **Normalization** and **Standarization** \n",
    "Data is usually collected from different sources in different formats.\n",
    "Normalization and standardization are the processes of transforming data into a common format, allowing the researcher to make the meaningful comparison.\n",
    "\n",
    "**Normalization** is the process of transforming values of several variables into a similar range. Typical normalizations include scaling the variable from 0 to 1, scaling the variable so the variance is \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Example**\n",
    "\n",
    "To demonstrate normalization, let's say we want to scale the columns , \"suicides_no_UK\" and \"suicides_no_Greece\" at df1_merged data frame.\n",
    "We would like to normalize those variables so their value ranges from 0 to 1\n",
    "We replace original value by (original value)/(maximum value)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1)** Create a function of normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(number):\n",
    "    return (number)/number.max()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2)** Apply the function to the selected column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suicides_UK_normalised=normalize(df1_merged['suicides_no_UK'])\n",
    "suicides_UK_normalised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suicides_Greece_normalised=normalize(df1_merged['suicides_no_Greece'])\n",
    "suicides_Greece_normalised"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Standardization**\n",
    "Data standardization is also a term for a particular type of data normalization where we subtract the mean and divide by the standard deviation. The data is transformed into a common format with  the mean to be 0 and the variable changes from negative  to positive around the mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standarize_test(number):\n",
    "    return (number-number.mean())/number.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standarize_test(UK_suicides_per_year['suicides_no'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise and compare\n",
    " I want to make a comparison between the normalised values fo both countries and I use again the merged data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add the two columns of normalised to the df1_merged dataset and create another datasets which includes the statistics. Or we can create a new data set.\n",
    "\n",
    "Lets go with the first option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_merged['UK_suicides_normalised']= suicides_UK_normalised # Add a new column at the merged dataframe and add the series of normalisation data as we calculated above\n",
    "df1_merged['Greece_suicides_normalised']= suicides_Greece_normalised\n",
    "df1_merged.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the normalisation data and get insightful information comparing the two countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_merged.plot(kind= 'bar', x='year', y=['UK_suicides_normalised','Greece_suicides_normalised'])# we can add in the dataframe the plot()function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see that in general the % of the cases in the UK was higher until 2011. After 2011 the % cases in Greece were higher and it is due to the economic crisis starting fro 2010 and have the greatest impact to the society after 2011. \n",
    "With these kind of comparative plots you can visualise and give insightful information about political and socio-economic of different countries in the 101 one presented in this data. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping plots\n",
    "You can group similar plots in a single figure using subplots. The matplotlib.pyplot.figure() which is shortened to plt.figure() creates a space into which we will place all our plots. The parameter figsize tells Python how big to take this space. Each subplot is placed into the figure using its add_subplot method. The add_subplot method takes 3 parameters. The first denotes how many total rows of subplots there are, the second parameter refers to the total number of subplot columns, and the final parameter denotes which subplot your variable is referencing (left-to-right, top-to-bottom). Each subplot is stored in a different variable (axes1, axes2, axes3). Once a subplot is created, the axes can be titled using the ax1.set_title() or set_xlabel() command (or set_ylabel()). Here are our four plots side by side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(12.0, 8.0))\n",
    "\n",
    "\n",
    "ax1= fig.add_subplot(2,2,1)\n",
    "#Here we plot bar using the plt.bar() function and it is set at the ax1 which is axes1 subplot variable\n",
    "plt.title('UK suicides') \n",
    "plt.bar(UK_data['year'],\n",
    "        UK_data['suicides_no'],\n",
    "        color='blue')\n",
    "\n",
    "\n",
    "ax2= fig.add_subplot(2,2,2)\n",
    "ax2.set_title('GDP in billions USD')\n",
    "ax2.plot(UK_data['year'],UK_data['gdp_for_year_usd']/10**9, color='purple' )\n",
    "\n",
    "\n",
    "ax3= fig.add_subplot(2,2,3)\n",
    "\n",
    "plt.title('Greece suicides')\n",
    "plt.bar(Greece_data['year'],\n",
    "        Greece_data['suicides_no'],\n",
    "        color='orange')\n",
    "\n",
    "\n",
    "ax4= fig.add_subplot(2,2,4)\n",
    "ax4.set_title('GDP in billions USD')\n",
    "ax4.plot(Greece_data['year'],Greece_data['gdp_for_year_usd']/10**9, color='green' )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to plot the GDP per capita over the years for both countries\n",
    "I need to calculate it by dividing the two columns and add the result as an extra column."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot the sum of the suicides over the years for all countries (101)over the years.  \n",
    "\n",
    "We could write it in one line: From the dataframe, group by the countries column, summarise and sort the values of suicides and plot the suicides_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "data_frame.groupby('country').sum().sort_values(by='suicides_no',ascending=False)[['suicides_no']][:101].plot(kind='bar',figsize=(16,8),title='Sum of suicides by country during 1985-2016')\n",
    "plt.ylabel(\"Sum of suicides\")\n",
    "plt.xlabel(\"Country\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here it plots the top 20 countries in suicides \n",
    "plt.figure(figsize=(14,6))\n",
    "data_frame.groupby('country').sum().sort_values(by='suicides_no',ascending=False)[['suicides_no']][:20].plot(kind='bar',figsize=(16,8),color='purple', title='Sum of suicides of the top 20 countries during 30 years 1985-2016')\n",
    "plt.ylabel(\"Sum of suicides\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Exercise \n",
    "Calculate the GDP/capita of the countries and visualise the cases according to GDP/capita.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to GDP/capita"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thank you for completing this lab!\n",
    "\n",
    "## Author\n",
    "##  Mary Tziraki\n",
    "\n",
    "\n",
    "                                                                                     \n",
    "\n",
    "## <h3 align=\"center\"> Â© Health+Bioscience IDEAS 2022. All rights reserved. <h3/>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "a0d4121080b5f33a2f2ddb6a1f46c0838d0d44af1b5f7e501e0b1477e85aebe3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
